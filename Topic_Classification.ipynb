{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgLcv98Qnlb9",
        "outputId": "4805d0c6-8b3e-41bc-b075-a89efc5fbcf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Machine learning (ML) is a field devoted to understanding and building methods that let machines \"learn\" – that is, methods that leverage data to improve computer performance on some set of tasks. It is seen as a broad subfield of artificial intelligence. \\nMachine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.In its application across business problems, machine learning is also referred to as predictive analytics.\\n\\nOverview\\nLearning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can sometimes be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". Other times, they can be more nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\".Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer\\'s part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.\\n\\nHistory and relationships to other fields\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.By the early 1960s an experimental \"learning machine\" with punched tape memory, called CyberTron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".Modern-day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\\n\\nArtificial intelligence\\nAs a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.:\\u200a488\\u200aHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.:\\u200a488\\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.:\\u200a708–710,\\u200a755\\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.:\\u200a25\\u200aMachine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.\\n\\nData mining\\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\\n\\nMachine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).\\n\\nGeneralization\\nThe difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\\n\\nStatistics\\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.\\n\\nPhysics\\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.\\n\\nTheory\\nA core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\\nFor the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\\n\\nApproaches\\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\\n\\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that\\'s analogous to rewards, which it tries to maximize.\\n\\nSupervised learning\\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.\\nSimilarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\\n\\nUnsupervised learning\\nUnsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function. Though unsupervised learning encompasses other domains involving summarizing and explaining data features.\\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\\n\\nSemi-supervised learning\\nSemi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\\n\\nReinforcement learning\\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\\n\\nDimensionality reduction\\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). This results in a smaller dimension of data (2D instead of 3D), while keeping all original variables in the model without changing the data.\\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.\\n\\nOther types\\nOther approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.As of 2022, deep learning is the dominant approach for much ongoing work in the field of machine learning.\\n\\nSelf-learning\\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \\n\\nin situation s perform action a\\nreceive consequence situation s\\'\\ncompute emotion of being in consequence situation v(s\\')\\nupdate crossbar memory  w\\'(a,s) = w(a,s) + v(s\\')It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.\\n\\nFeature learning\\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\\n\\nSparse dictionary learning\\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\\n\\nAnomaly detection\\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\\n\\nRobot learning\\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).\\n\\nAssociation rules\\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \\n  \\n    \\n      \\n        {\\n        \\n          o\\n          n\\n          i\\n          o\\n          n\\n          s\\n          ,\\n          p\\n          o\\n          t\\n          a\\n          t\\n          o\\n          e\\n          s\\n        \\n        }\\n        ⇒\\n        {\\n        \\n          b\\n          u\\n          r\\n          g\\n          e\\n          r\\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{\\\\mathrm {onions,potatoes} \\\\}\\\\Rightarrow \\\\{\\\\mathrm {burger} \\\\}}\\n   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\\n\\nModels\\nPerforming machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.\\n\\nArtificial neural networks\\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\\n\\nDecision trees\\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item\\'s target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\\n\\nSupport-vector machines\\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\\n\\nRegression analysis\\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\\n\\nBayesian networks\\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\\n\\nGaussian processes\\nA Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\\nGiven a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\\nGaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.\\n\\nGenetic algorithms\\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\\n\\nTraining models\\nTypically, machine learning models require a high quantity of reliable data in order for the models to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Bias models may result in detrimental outcomes thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably be integrated within machine learning engineering teams.\\n\\nFederated learning\\nFederated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users\\' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users\\' mobile phones without having to send individual searches back to Google.\\n\\nApplications\\nThere are many applications for machine learning, including:\\n\\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers\\' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behavior of travelers. Recently, machine learning technology was also applied to optimize smartphone\\'s performance and thermal behavior based on the user\\'s interaction with the phone.\\n\\nLimitations\\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.\\n\\nBias\\nMachine learning approaches in particular can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. Language models learned from data have been shown to contain human-like biases. Machine learning systems used for criminal risk assessment have been found to be biased against black people. In 2015, Google photos would often tag black people as gorillas, and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data, and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that \"There\\'s nothing artificial about AI...It\\'s inspired by people, it\\'s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\\n\\nExplainability\\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\\n\\nOverfitting\\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.\\n\\nOther limitations and vulnerabilities\\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. Some systems are so brittle that changing a single adversarial pixel predictably induces misclassification. Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models which are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.\\n\\nModel assessments\\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model\\'s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC\\'s associated area under the curve (AUC).\\n\\nEthics\\nMachine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, in 1988, the UK\\'s Commission for Racial Equality found that St. George\\'s Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to be either women or had non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.\\nAI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on the objectivity and logical reasoning. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public\\'s interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increase profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm\\'s proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\\n\\nHardware\\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\\n\\nNeuromorphic/Physical Neural Networks\\nA physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.\\n\\nEmbedded Machine Learning\\nEmbedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration, using approximate computing, optimization of machine learning models and many more.\\n\\nSoftware\\nSoftware suites containing a variety of machine learning algorithms include the following:\\n\\nFree and open-source software\\nProprietary software with free and open-source editions\\nKNIME\\nRapidMiner\\n\\nProprietary software\\nJournals\\nJournal of Machine Learning Research\\nMachine Learning\\nNature Machine Intelligence\\nNeural Computation\\nIEEE Transactions on Pattern Analysis and Machine Intelligence\\n\\nConferences\\nAAAI Conference on Artificial Intelligence\\nAssociation for Computational Linguistics (ACL)\\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\\nInternational Conference on Machine Learning (ICML)\\nInternational Conference on Learning Representations (ICLR)\\nInternational Conference on Intelligent Robots and Systems (IROS)\\nConference on Knowledge Discovery and Data Mining (KDD)\\nConference on Neural Information Processing Systems (NeurIPS)\\n\\nSee also\\nAutomated machine learning – Process of automating the application of machine learning\\nBig data – Information assets characterized by high volume, velocity, and variety\\nDifferentiable programming – Programming paradigm\\nList of important publications in machine learning\\nList of datasets for machine-learning research\\n\\nReferences\\nSources\\nDomingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\\n\\nFurther reading\\nExternal links\\n\\n Quotations related to Machine learning at Wikiquote\\nInternational Machine Learning Society\\nmloss is an academic database of open-source machine learning software.', 'Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by non-human animals or by humans. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs.\\nAI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.Artificial intelligence was founded as an academic discipline in 1956, and in the years since it has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success, and renewed funding. AI research has tried and discarded many different approaches, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge, and imitating animal behavior. In the first decades of the 21st century, highly mathematical and statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field\\'s long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.\\nThe field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals. The term artificial intelligence has also been criticized for overhyping AI\\'s true technological capabilities.\\n\\nHistory\\nArtificial beings with intelligence appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley\\'s Frankenstein or Karel Čapek\\'s R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing\\'s theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis. This, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts\\' 1943 formal design for Turing-complete \"artificial neurons\".By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known as Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the \"heuristic search\" approach, which likened intelligence to a problem of exploring a space of possibilities for answers.\\nThe second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons. James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.The field of AI research was born at a workshop at Dartmouth College in 1956. The attendees became the founders and leaders of AI research. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field. Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\".They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan\\'s fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.Interest in neural networks and \"connectionism\" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s. Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.\\nAI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012. According to Bloomberg\\'s Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.In a 2017 survey, one in five companies reported they had \"incorporated AI in some offerings or processes\". The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.In April 2023, computer scientist Jaron Lanier published an alternative view of AI in The New Yorker as less intelligent than the name, and popular culture, may suggest. Lanier concludes his essay as follows: \"Think of people. People are the answer to the problems of bits.\"\\n\\nGoals\\nThe general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.\\n\\nReasoning, problem-solving\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a \"combinatorial explosion\": they became exponentially slower as the problems grew larger. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.\\n\\nKnowledge representation\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts.\\nA representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know);. default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\\n\\nLearning\\nMachine learning (ML), a fundamental concept of AI research since the field\\'s inception, is the study of computer algorithms that improve automatically through experience.Unsupervised learning finds patterns in a stream of input.\\nSupervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in – the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as \"function approximators\" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, \"spam\" or \"not spam\".In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.Transfer learning is when the knowledge gained from one problem is applied to a new problem.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\\n\\nNatural language processing\\nNatural language processing (NLP)\\nallows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.\\nSymbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic and the breadth of commonsense knowledge. Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), \"Keyword spotting\" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others. They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.\\n\\nPerception\\nMachine perception\\nis the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,facial recognition, and object recognition.\\nComputer vision is the ability to analyze visual input.\\n\\nSocial intelligence\\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.\\nFor example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\\nHowever, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.\\n\\nGeneral intelligence\\nA machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, \"master algorithm\" that could lead to AGI.\\nOthers believe that anthropomorphic features like an artificial brain\\nor simulated child development\\nwill someday reach a critical point where general intelligence emerges.\\n\\nTools\\nSearch and optimization\\nAI can solve many problems by intelligently searching through many possible solutions. Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.Simple exhaustive searches\\nare rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies.\\nHeuristics limit the search for solutions into a smaller sample size.\\n\\nA very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing. Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming. Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\\n\\nLogic\\nLogic\\nis used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning\\nand inductive logic programming is a method for learning.Several different forms of logic are used in AI research. Propositional logic involves truth functions such as \"or\" and \"not\". First-order logic\\nadds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a \"degree of truth\" (between 0 and 1) to vague statements such as \"Alice is old\" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.\\nSeveral extensions of logic have been designed to handle specific domains of knowledge, such as description logics;situation calculus, event calculus and fluent calculus (for representing events and time);causal calculus;belief calculus (belief revision); and modal logics.\\nLogics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.\\n\\nProbabilistic methods for uncertain reasoning\\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.Bayesian networks\\nare a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks) and perception (using dynamic Bayesian networks).\\nProbabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).A key concept from the science of economics is \"utility\", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,\\nand information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\\n\\nClassifiers and statistical learning methods\\nThe simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if diamond then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.A classifier can be trained in various ways; there are many statistical and machine learning approaches.\\nThe decision tree is the simplest and most widely used symbolic machine learning algorithm.K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.Neural networks are also used for classification.Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as \"naive Bayes\" on most practical data sets.\\n\\nArtificial neural networks\\nNeural networks\\nwere inspired by the architecture of neurons in the human brain. A simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), casts a weighted \"vote\" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.\\nModern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.\\nOther learning techniques for neural networks are Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning.The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.\\n\\nDeep learning\\nDeep learning\\nuses several layers of neurons between the network\\'s inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.\\nDeep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron\\'s receptive field. This can substantially reduce the number of weighted connections between neurons, and creates a hierarchy similar to the organization of the animal visual cortex.In a recurrent neural network (RNN) the signal will propagate through a layer more than once;\\nthus, an RNN is an example of deep learning.\\nRNNs can be trained by gradient descent,\\nhowever long-term gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), known as the vanishing gradient problem.\\nThe long short term memory (LSTM) technique can prevent this in most cases.\\n\\nSpecialized languages and hardware\\nSpecialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing.\\n\\nApplications\\nAI is relevant to any intellectual task.\\nModern artificial intelligence techniques are pervasive and are too numerous to list here.\\nFrequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),\\ntargeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon),\\ndriving internet traffic, targeted advertising (AdSense, Facebook),\\nvirtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars),\\nautomatic language translation (Microsoft Translator, Google Translate),\\nfacial recognition (Apple\\'s Face ID or Microsoft\\'s DeepFace),\\nimage labeling (used by Facebook, Apple\\'s iPhoto and TikTok)\\n, spam filtering and chatbots (such as Chat GPT).\\nThere are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage, deepfakes, medical diagnosis, military logistics, foreign policy, or supply chain management.\\nGame playing has been a test of AI\\'s strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM\\'s question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.\\nIn March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus and Cepheus. DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own.By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.\\nDeepMind\\'s AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.\\nOther applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.\\nAI content detector tools are software applications that use artificial intelligence (AI) algorithms to analyze and detect specific types of content in digital media, such as text, images, and videos. These tools are commonly used to identify inappropriate content, such as speech errors, violent or sexual images, and spam, among others.\\nSome benefits of using AI content detector tools include improved efficiency and accuracy in detecting inappropriate content, increased safety and security for users, and reduced legal and reputational risks for websites and platforms.\\n\\nSmart traffic lights\\nSmart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.\\n\\nIntellectual property\\nIn 2019, WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G). Since AI emerged in the 1950s, 340,000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four. The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134,777 machine learning patents filed for a total of 167,038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.\\n\\nPhilosophy\\nDefining artificial intelligence\\nAlan Turing wrote in 1950 \"I propose to consider the question \\'can machines think\\'?\"\\nHe advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".\\nHe devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks\"Russell and Norvig agree with Turing that AI must be defined in terms of \"acting\" and not \"thinking\". However, they are critical that the test compares machines to people. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making \\'machines that fly so exactly like pigeons that they can fool other pigeons.\\'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world.\" Another AI founder, Marvin Minsky similarly defines it as \"the ability to solve hard problems\". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\\nA definition that has also been adopted by Google - major practitionary in the field of AI.\\nThis definition stipulated the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\\n\\nEvaluating approaches to AI\\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.\\n\\nSymbolic AI and its limits\\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec\\'s paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.\\nPhilosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.\\nAlthough his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\\n\\nNeat vs. scruffy\\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems (especially in areas like common sense reasoning). This issue was actively discussed in the 70s and 80s,\\nbut in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed \"the victory of the neats\".\\n\\nSoft vs. hard computing\\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\\n\\nNarrow vs. general AI\\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field\\'s long-term goals.\\nGeneral intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.\\n\\nMachine consciousness, sentience and mind\\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers \"don\\'t care about the [philosophy of AI] – as long as the program works, they don\\'t care whether you call it a simulation of intelligence or real intelligence.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\\n\\nConsciousness\\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\\n\\nComputationalism and functionalism\\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\\nSearle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\\n\\nRobot rights\\nIf a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.\\nAny hypothetical robot rights would lie on a spectrum with animal rights and human rights.\\nThis issue has been considered in fiction for centuries,\\nand is now being considered by, for example, California\\'s Institute for the Future; however, critics argue that the discussion is premature.\\n\\nFuture\\nSuperintelligence\\nA superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.\\nIts intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the \"singularity\".\\nBecause it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler\\'s \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\\n\\nRisks\\nTechnological unemployment\\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we\\'re in uncharted territory\" with AI.\\nA survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.\\nSubjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\".Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".\\nJobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\\n\\nBad actors and weaponized AI\\nAI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.\\n\\nAlgorithmic bias\\nAI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.\\nBias can be inadvertently introduced by the way training data is selected.\\nIt can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.Health equity issues may also be exacerbated when many-to-many mapping are done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage. Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.\\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\\n\\nExistential risk\\nSuperintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, \"spell the end of the human race\". Philosopher Nick Bostrom argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI\\'s goals do not fully reflect humanity\\'s, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or \"friendly\" its stated goals might be.\\nPolitical scientist Charles T. Rubin argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.The opinion of experts and industry insiders is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.Stephen Hawking, Microsoft founder Bill Gates, history professor Yuval Noah Harari, and SpaceX founder Elon Musk have all expressed serious misgivings about the future of AI.\\nProminent tech titans including Peter Thiel (Amazon Web Services) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as OpenAI and the Future of Life Institute.Mark Zuckerberg (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.\\nOther experts argue is that the risks are far enough in the future to not be worth researching,\\nor that humans will be valuable from the perspective of a superintelligent machine.Rodney Brooks, in particular, has said that \"malevolent\" AI is still centuries away.\\n\\nCopyright\\nAI\\'s decisions making abilities raises the questions of legal responsibility and copyright status of created works. This issues are being refined in various jurisdictions.\\n\\nEthical machines\\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\\nMachine ethics is also called machine morality, computational ethics or computational morality,\\nand was founded at an AAAI symposium in 2005.Other approaches include Wendell Wallach\\'s \"artificial moral agents\"\\nand Stuart J. Russell\\'s three principles for developing provably beneficial machines.\\n\\nRegulation\\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.\\nThe regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.\\nBetween 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.\\nMost EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.\\nThe Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.\\n\\nIn fiction\\nThought-capable artificial beings have appeared as storytelling devices since antiquity,\\nand have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley\\'s Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke\\'s and Stanley Kubrick\\'s 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov\\'s laws are often brought up during lay discussions of machine ethics;\\nwhile almost all artificial intelligence researchers are familiar with Asimov\\'s laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.\\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek\\'s R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\\n\\nSee also\\nAI safety – Research area on making AI safe and beneficial\\nAI alignment – Conformance to the intended objective\\nArtificial intelligence in healthcare - Machine-learning algorithms and software in the analysis, presentation, and comprehension of complex medical and health care data\\nArtificial intelligence arms race – Arms race for the most advanced AI-related technologies\\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\\nBusiness process automation – Technology-enabled automation of complex business processes\\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\\nEmergent algorithm – Algorithm exhibiting emergent behavior\\nFemale gendering of AI technologies – Design of digital assistants as female\\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\\nOperations research – Discipline concerning the application of advanced analytical methods\\nRobotic process automation – Form of business process automation technology\\nSynthetic intelligence – Alternate term for or form of artificial intelligence\\nUniversal basic income – Welfare system of unconditional income\\nWeak artificial intelligence – Form of artificial intelligence\\nData sources – The list of data sources for study and research\\n\\nExplanatory notes\\nReferences\\nAI textbooks\\nThese were the four the most widely used AI textbooks in 2008:\\n\\nHistory of AI\\nOther sources\\nFurther reading\\nExternal links\\n\\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\\nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).', 'Deep learning is part of a broader family of machine learning methods, which is based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability.\\n\\nDefinition\\nDeep learning is a class of machine learning algorithms that:\\u200a199–200\\u200a uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\\nFrom another angle to view deep learning, deep learning refers to ‘computer-simulate’ or ‘automate’ human learning processes from a source (e.g., an image of dogs) to a learned object (dogs). Therefore, a notion coined as “deeper” learning or “deepest” learning  makes sense. The deepest learning refers to the fully automatic learning from a source to a final learned object. A deeper learning thus refers to a mixed learning process: a human learning process from a source to a learned semi-object, followed by a computer learning process from the human learned semi-object to a final learned object.\\n\\nOverview\\nMost modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.\\n\\nInterpretations\\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima\\'s rectified linear unit.The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\\nThe probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\\n\\nHistory\\nThere are two types of neural networks: feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don\\'t. In the 1920s, Wilhelm Lenz and Ernst Ising created and analyzed the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun\\'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. RNNs have become central for speech recognition and language processing.\\nCharles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today, referring to Rosenblatt\\'s 1962 book which introduced a multilayer perceptron (MLP) with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. However, since only the output layer had learning connections, this was not yet deep learning. It was what later was called an extreme learning machine.The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\\'ichi Amari. In computer experiments conducted by Amari\\'s student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.In 1970, Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. \\nThe terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985, David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layersbegan with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969, he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision.\\nThe term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.In 1988, Wei Zhang et al. applied the backpropagation algorithm \\nto a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. \\nIn 1989, Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. \\nSubsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\\nIn the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, Juergen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a chunker solved a deep learning task whose depth exceeded 1000.In 1992, Juergen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern.  \\nThe modern Transformer was introduced by Ashish Vaswani et. al. in their 2017 paper \"Attention Is All You Need.\" \\nIt combines this with a softmax operator and a projection matrix.\\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.In 1991, Juergen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network\\'s gain is the other network\\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity.\" \\nIn 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\\'s output is in a given set. This can be used to create realistic deepfakes.\\nExcellent image quality is achieved by Nvidia\\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et. al. Here the GAN generator is grown from small to large scale in a pyramidal fashion.\\nSepp Hochreiter\\'s diploma thesis (1991) was called \"one of the most important documents in the history of machine learning\" by his supervisor Schmidhuber. It not only tested the neural history compressor, but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM), published in 1997. LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.\\nIn 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.Since 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.\\nSimpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network\\'s (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.\\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.\\nMost speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\\'s NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.\\nThe principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google\\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.\\nIn 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.\\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.\\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks (CNNs) were superseded for ASR by CTC for LSTM. but are more successful in computer vision.\\nAdvances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).” That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.\\n\\nDeep learning revolution\\nIn the late 2000s, deep learning started to outperform other methods in machine learning competitions.\\nIn 2009, a long short-term memory trained by connectionist temporal classification (Alex Graves, Santiago Fernández, Faustino Gomez, and Juergen Schmidhuber, 2006) was the first RNN to win pattern recognition contests, winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011, the DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Juergen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. Also in 2011, DanNet won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.  In September 2012, DanNet also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012, the similar AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. \\nThe VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and\\nwon the ImageNet 2014 competition, following a similar trend in large-scale speech recognition. \\nImage classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.In 2012, a team led by George E. Dahl won the \"Merck Molecular Activity Challenge\" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014, Sepp Hochreiter\\'s group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the \"Tox21 Data Challenge\" of NIH, FDA and NCATS.In 2016, Roger Parloff mentioned a \"deep learning revolution\" that has transformed the AI industry.In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.\\n\\nNeural networks\\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\\nAn ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\" ).\\n\\nDeep neural networks\\nA deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function similarly to a human brain, and can be trained like any other ML algorithm.For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks.\\nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\\nRecurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.Convolutional deep neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).\\n\\nChallenges\\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko\\'s unit pruning or weight decay (\\n  \\n    \\n      \\n        \\n          ℓ\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\ell _{2}}\\n  -regularization) or sparsity (\\n  \\n    \\n      \\n        \\n          ℓ\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\ell _{1}}\\n  -regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn\\'t require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.\\n\\nHardware\\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.\\n\\nApplications\\nAutomatic speech recognition\\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\\n\\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:\\nScale-up/out and accelerated DNN training and decoding\\nSequence discriminative training\\nFeature processing by deep models with solid understanding of the underlying mechanisms\\nAdaptation of DNNs and related deep models\\nMulti-task and transfer learning by DNNs and related deep models\\nCNNs and how to design them to best exploit domain knowledge of speech\\nRNN and its rich LSTM variants\\nOther types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.\\n\\nImage recognition\\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\\n\\nVisual art processing\\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\\n\\nidentifying the style period of a given painting\\nNeural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video\\ngenerating striking imagery based on random visual input fields.\\n\\nNatural language processing\\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, Text classification and others.Recent developments generalize word embedding to sentence embedding.\\nGoogle Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples.\" It translates \"whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs.\\n\\nDrug discovery and toxicology\\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.\\n\\nCustomer relationship management\\nDeep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.\\n\\nRecommendation systems\\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\\n\\nBioinformatics\\nAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.\\n\\nMedical image analysis\\nDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.\\n\\nMobile advertising\\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\\n\\nImage restoration\\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\\n\\nFinancial fraud detection\\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.\\n\\nMilitary\\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation.\\n\\nPartial differential equations\\nPhysics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods relies on.\\n\\nImage Reconstruction\\nImage reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging  and ultrasound imaging.Epigenetic clock\\nFor more information, see Epigenetic clock.\\nAn epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock is planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.\\n\\nRelation to human cognitive and brain development\\nDeep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant\\'s brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.\"A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.\\n\\nCommercial activity\\nFacebook\\'s AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.Google\\'s DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.\\nIn 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”\\n\\nCriticism and comment\\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\\n\\nTheory\\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.\\nOthers point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:\"Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.\"\\nIn further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian\\'s website.\\nFurthermore, while deep learning consists of dozens and even hundreds of layers, the brain itself consists of very few layers. Simulations on shallow networks, which are closer to the brain dynamics, indicate a similar performance as deep learning with a lower complexity.\\n\\nErrors\\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).\\n\\nCyber threat\\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an “adversarial attack.”In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another\\'s focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware).\"In “data poisoning,” false data is continually smuggled into a machine learning system\\'s training set to prevent it from achieving mastery.\\n\\nReliance on human microwork\\nMost Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook\\'s face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether of not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture. This user interface is a mechanism to generate \"a constant stream of verification data\" to further train the network in real-time. As Mühlhoff argues, involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as \"human-aided artificial intelligence\".\\n\\nSee also\\nApplications of artificial intelligence\\nComparison of deep learning software\\nCompressed sensing\\nDifferentiable programming\\nEcho state network\\nList of artificial intelligence projects\\nLiquid state machine\\nList of datasets for machine-learning research\\nReservoir computing\\nScale space and deep learning\\nSparse coding\\n\\nReferences\\n\\n\\n== Further reading ==', 'Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labeled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n\\nSteps to follow\\nTo solve a given problem of supervised learning, one has to perform the following steps:\\n\\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting or perhaps a full paragraph of handwriting.\\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.\\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.\\n\\nAlgorithm choice\\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\\nThere are four major issues to consider in supervised learning:\\n\\nBias-variance tradeoff\\nA first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . A learning algorithm has high variance for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\\n\\nFunction complexity and amount of training data\\nThe second issue is of the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \"flexible\" learning algorithm with low bias and high variance.\\n\\nDimensionality of the input space\\nA third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\\n\\nNoise in the output values\\nA fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\\nIn practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.\\n\\nOther factors to consider\\nOther factors to consider when choosing and applying a learning algorithm include the following:\\n\\nHeterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including support-vector machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support-vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data.\\nRedundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.\\nPresence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, support-vector machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support-vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\\n\\nAlgorithms\\nThe most widely used learning algorithms are: \\n\\nSupport-vector machines\\nLinear regression\\nLogistic regression\\nNaive Bayes\\nLinear discriminant analysis\\nDecision trees\\nK-nearest neighbor algorithm\\nNeural networks (Multilayer perceptron)\\nSimilarity learning\\n\\nHow supervised learning algorithms work\\nGiven a set of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   training examples of the form \\n  \\n    \\n      \\n        {\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            1\\n          \\n        \\n        )\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        (\\n        \\n          x\\n          \\n            N\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            N\\n          \\n        \\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{(x_{1},y_{1}),...,(x_{N},\\\\;y_{N})\\\\}}\\n   such that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is the feature vector of the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th example and \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   is its label (i.e., class), a learning algorithm seeks a function \\n  \\n    \\n      \\n        g\\n        :\\n        X\\n        →\\n        Y\\n      \\n    \\n    {\\\\displaystyle g:X\\\\to Y}\\n  , where \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   is the input space and \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n   is the output space. The function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is an element of some space of possible functions \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  , usually called the hypothesis space. It is sometimes convenient to represent \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   using a scoring function \\n  \\n    \\n      \\n        f\\n        :\\n        X\\n        ×\\n        Y\\n        →\\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle f:X\\\\times Y\\\\to \\\\mathbb {R} }\\n   such that \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is defined as returning the \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   value that gives the highest score: \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              arg\\n              \\u2061\\n              max\\n            \\n            y\\n          \\n        \\n        \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)={\\\\underset {y}{\\\\arg \\\\max }}\\\\;f(x,y)}\\n  . Let \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   denote the space of scoring functions.\\nAlthough \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   and \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   can be any space of functions, many learning algorithms are probabilistic models where \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   takes the form of a conditional probability model \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)=P(y|x)}\\n  , or \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   takes the form of a joint probability model \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        P\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x,y)=P(x,y)}\\n  . For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\\nThere are two basic approaches to choosing \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   or \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  : empirical risk minimization and structural risk minimization. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.\\nIn both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},\\\\;y_{i})}\\n  . In order to measure how well a function fits the training data, a loss function \\n  \\n    \\n      \\n        L\\n        :\\n        Y\\n        ×\\n        Y\\n        →\\n        \\n          \\n            R\\n          \\n          \\n            ≥\\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L:Y\\\\times Y\\\\to \\\\mathbb {R} ^{\\\\geq 0}}\\n   is defined. For training example \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},\\\\;y_{i})}\\n  , the loss of predicting the value \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   is \\n  \\n    \\n      \\n        L\\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle L(y_{i},{\\\\hat {y}})}\\n  .\\nThe risk \\n  \\n    \\n      \\n        R\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle R(g)}\\n   of function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is defined as the expected loss of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  . This can be estimated from the training data as\\n\\n  \\n    \\n      \\n        \\n          R\\n          \\n            e\\n            m\\n            p\\n          \\n        \\n        (\\n        g\\n        )\\n        =\\n        \\n          \\n            1\\n            N\\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        L\\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        ,\\n        g\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle R_{emp}(g)={\\\\frac {1}{N}}\\\\sum _{i}L(y_{i},g(x_{i}))}\\n  .\\n\\nEmpirical risk minimization\\nIn empirical risk minimization, the supervised learning algorithm seeks the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that minimizes \\n  \\n    \\n      \\n        R\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle R(g)}\\n  . Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  .\\nWhen \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is a conditional probability distribution \\n  \\n    \\n      \\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle P(y|x)}\\n   and the loss function is the negative log likelihood: \\n  \\n    \\n      \\n        L\\n        (\\n        y\\n        ,\\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        )\\n        =\\n        −\\n        log\\n        \\u2061\\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle L(y,{\\\\hat {y}})=-\\\\log P(y|x)}\\n  , then empirical risk minimization is equivalent to maximum likelihood estimation.\\nWhen \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.\\n\\nStructural risk minimization\\nStructural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam\\'s razor that prefers simpler functions over more complex ones.\\nA wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is a linear function of the form\\n\\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          ∑\\n          \\n            j\\n            =\\n            1\\n          \\n          \\n            d\\n          \\n        \\n        \\n          β\\n          \\n            j\\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle g(x)=\\\\sum _{j=1}^{d}\\\\beta _{j}x_{j}}\\n  .A popular regularization penalty is \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            j\\n          \\n        \\n        \\n          β\\n          \\n            j\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{j}\\\\beta _{j}^{2}}\\n  , which is the squared Euclidean norm of the weights, also known as the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{2}}\\n   norm. Other norms include the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{1}}\\n   norm, \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n        \\n          β\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{j}|\\\\beta _{j}|}\\n  , and the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{0}}\\n   \"norm\", which is the number of non-zero \\n  \\n    \\n      \\n        \\n          β\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\beta _{j}}\\n  s. The penalty will be denoted by \\n  \\n    \\n      \\n        C\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle C(g)}\\n  .\\nThe supervised learning optimization problem is to find the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that minimizes\\n\\n  \\n    \\n      \\n        J\\n        (\\n        g\\n        )\\n        =\\n        \\n          R\\n          \\n            e\\n            m\\n            p\\n          \\n        \\n        (\\n        g\\n        )\\n        +\\n        λ\\n        C\\n        (\\n        g\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle J(g)=R_{emp}(g)+\\\\lambda C(g).}\\n  The parameter \\n  \\n    \\n      \\n        λ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   controls the bias-variance tradeoff. When \\n  \\n    \\n      \\n        λ\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\lambda =0}\\n  , this gives empirical risk minimization with low bias and high variance. When \\n  \\n    \\n      \\n        λ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   is large, the learning algorithm will have high bias and low variance. The value of \\n  \\n    \\n      \\n        λ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   can be chosen empirically via cross validation.\\nThe complexity penalty has a Bayesian interpretation as the negative log prior probability of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  , \\n  \\n    \\n      \\n        −\\n        log\\n        \\u2061\\n        P\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle -\\\\log P(g)}\\n  , in which case \\n  \\n    \\n      \\n        J\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle J(g)}\\n   is the posterior probability of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  .\\n\\nGenerative training\\nThe training methods described above are discriminative training methods, because they seek to find a function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that discriminates well between the different output values (see discriminative model). For the special case where \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        P\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x,y)=P(x,y)}\\n   is a joint probability distribution and the loss function is the negative log likelihood \\n  \\n    \\n      \\n        −\\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        log\\n        \\u2061\\n        P\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle -\\\\sum _{i}\\\\log P(x_{i},y_{i}),}\\n   a risk minimization algorithm is said to perform generative training, because \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\\n\\nGeneralizations\\nThere are several ways in which the standard supervised learning problem can be generalized:\\n\\nSemi-supervised learning: In this setting, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled.\\nWeak supervision: In this setting, noisy, limited, or imprecise sources are used to provide supervision signal for labeling training data.\\nActive learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.\\nStructured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.\\nLearning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.\\n\\nApproaches and algorithms\\nAnalytical learning\\nArtificial neural network\\nBackpropagation\\nBoosting (meta-algorithm)\\nBayesian statistics\\nCase-based reasoning\\nDecision tree learning\\nInductive logic programming\\nGaussian process regression\\nGenetic programming\\nGroup method of data handling\\nKernel estimators\\nLearning automata\\nLearning classifier systems\\nLearning vector quantization\\nMinimum message length (decision trees, decision graphs, etc.)\\nMultilinear subspace learning\\nNaive Bayes classifier\\nMaximum entropy classifier\\nConditional random field\\nNearest neighbor algorithm\\nProbably approximately correct learning (PAC) learning\\nRipple down rules, a knowledge acquisition methodology\\nSymbolic machine learning algorithms\\nSubsymbolic machine learning algorithms\\nSupport vector machines\\nMinimum complexity machines (MCM)\\nRandom forests\\nEnsembles of classifiers\\nOrdinal classification\\nData pre-processing\\nHandling imbalanced datasets\\nStatistical relational learning\\nProaftn, a multicriteria classification algorithm\\n\\nApplications\\nBioinformatics\\nCheminformatics\\nQuantitative structure–activity relationship\\nDatabase marketing\\nHandwriting recognition\\nInformation retrieval\\nLearning to rank\\nInformation extraction\\nObject recognition in computer vision\\nOptical character recognition\\nSpam detection\\nPattern recognition\\nSpeech recognition\\nSupervised learning is a special case of downward causation in biological systems\\nLandform classification using satellite imagery\\nSpend classification in procurement processes\\n\\nGeneral issues\\nComputational learning theory\\nInductive bias\\nOverfitting (machine learning)\\n(Uncalibrated) class membership probabilities\\nUnsupervised learning\\nVersion spaces\\n\\nSee also\\nList of datasets for machine learning research\\n\\nReferences\\nExternal links\\nMachine Learning Open Source Software (MLOSS)', 'Unsupervised learning  is a type of algorithm that learns patterns from untagged data. The goal is that through mimicry, which is an important mode of learning in people, the machine is forced to build a concise representation of its world and then generate imaginative content from it.\\nIn contrast to supervised learning where data is tagged by an expert, e.g. tagged as a \"ball\" or \"fish\", unsupervised methods exhibit self-organization that captures patterns as probability densities or a combination of neural feature preferences encoded in the machine\\'s weights and activations. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a numerical performance score as guidance, and semi-supervised learning where a small portion of the data is tagged.\\n\\nNeural networks\\nTasks vs. methods\\nNeural network tasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.  For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, relu, and adaptive learning rates.\\n\\nTraining\\nDuring the learning phase, an unsupervised network tries to mimic the data it\\'s given and uses the error in its mimicked output to correct itself (i.e. correct its weights & biases). Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be expressed as an unstable high energy state in the network.\\nIn contrast to supervised methods\\' dominant use of backpropagation, unsupervised learning also employs other methods  including:  Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations.  See the table below for more details.\\n\\nEnergy\\nAn energy function is a macroscopic measure of a network\\'s activation state.  In Boltzmann machines, it plays the role of the Cost function.  This analogy with physics is inspired by Ludwig Boltzmann\\'s analysis of a gas\\' macroscopic energy from the microscopic probabilities of particle motion \\n  \\n    \\n      \\n        p\\n        ∝\\n        \\n          e\\n          \\n            −\\n            E\\n            \\n              /\\n            \\n            k\\n            T\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p\\\\propto e^{-E/kT}}\\n  , where k is the Boltzmann constant and T is temperature. In the RBM network the relation is \\n  \\n    \\n      \\n        p\\n        =\\n        \\n          e\\n          \\n            −\\n            E\\n          \\n        \\n        \\n          /\\n        \\n        Z\\n      \\n    \\n    {\\\\displaystyle p=e^{-E}/Z}\\n  , where \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   and \\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n   vary over every possible activation pattern and \\n  \\n    \\n      \\n        \\n          \\n            Z\\n            =\\n            \\n              ∑\\n              \\n                \\n                  \\n                    All Patterns\\n                  \\n                \\n              \\n            \\n            \\n              e\\n              \\n                −\\n                E\\n                (\\n                \\n                  pattern\\n                \\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle {Z=\\\\sum _{\\\\scriptscriptstyle {\\\\text{All Patterns}}}e^{-E({\\\\text{pattern}})}}}\\n  . To be more precise, \\n  \\n    \\n      \\n        p\\n        (\\n        a\\n        )\\n        =\\n        \\n          e\\n          \\n            −\\n            E\\n            (\\n            a\\n            )\\n          \\n        \\n        \\n          /\\n        \\n        Z\\n      \\n    \\n    {\\\\displaystyle p(a)=e^{-E(a)}/Z}\\n  , where \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   is an activation pattern of all neurons (visible and hidden). Hence, early neural networks bear the name Boltzmann Machine.  Paul Smolensky calls \\n  \\n    \\n      \\n        −\\n        E\\n        \\n      \\n    \\n    {\\\\displaystyle -E\\\\,}\\n   the Harmony. A network seeks low energy which is high Harmony.\\n\\nNetworks\\nThis table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Networks.  Circles are neurons and edges between them are connection weights.  As network design changes, features are added on to enable new capabilities or removed to make learning faster.  For instance, neurons change between deterministic (Hopfield) and stochastic (Boltzmann) to allow robust output, weights are removed within a layer (RBM) to hasten learning, or connections are allowed to become asymmetric (Helmholtz).\\n\\nOf the networks bearing people\\'s names, only Hopfield worked directly with neural networks.  Boltzmann and Helmholtz came before artificial neural networks, but their work in physics and physiology inspired the analytical methods that were used.\\n\\nHistory\\nSpecific Networks\\nHere, we highlight some characteristics of select networks.  The details of each are given in the comparison table below. \\n\\nHopfield Network\\nFerromagnetism inspired Hopfield networks.  A neuron correspond to an iron domain with binary magnetic moments Up and Down, and neural connections correspond to the domain\\'s influence on each other.  Symmetric connections enable a global energy formulation. During inference the network updates each state using the standard activation step function. Symmetric weights and the right energy functions guarantees convergence to a stable activation pattern.  Asymmetric weights are difficult to analyze.  Hopfield nets are used as Content Addressable Memories (CAM).\\n\\nBoltzmann Machine\\nThese are stochastic Hopfield nets. Their state value is sampled from this pdf as follows: suppose a binary neuron fires with the Bernoulli probability p(1) = 1/3 and rests with p(0) = 2/3. One samples from it by taking a UNIFORMLY distributed random number y, and plugging it into the inverted cumulative distribution function, which in this case is the step function thresholded at 2/3. The inverse function = { 0 if x <= 2/3, 1 if x > 2/3 }\\n\\nSigmoid Belief Net\\nIntroduced by Radford Neal in 1992, this network applies ideas from probabilistic graphical models to neural networks.  A key difference is that nodes in graphical models have pre-assigned meanings, whereas Belief Net neurons\\' features are determined after training. The network is a sparsely connected directed acyclic graph composed of binary stochastic neurons.  The learning rule comes from Maximum Likelihood on p(X):  Δwij \\n  \\n    \\n      \\n        ∝\\n      \\n    \\n    {\\\\displaystyle \\\\propto }\\n   sj * (si - pi), where pi = 1 / ( 1 + eweighted inputs into neuron i ).  sj\\'s are activations from an unbiased sample of the posterior distribution and this is problematic due to the Explaining Away problem raised by Judea Perl.  Variational Bayesian methods uses a surrogate posterior and blatantly disregard this complexity.\\n\\nDeep Belief Network\\nIntroduced by Hinton, this network is a hybrid of RBM and Sigmoid Belief Network.  The top 2 layers is an RBM and the second layer downwards form a sigmoid belief network.  One trains it by the stacked RBM method and then throw away the recognition weights below the top RBM.  As of 2009, 3-4 layers seems to be the optimal depth.\\n\\nHelmholtz machine\\nThese are early inspirations for the Variational Auto Encoders. It\\'s 2 networks combined into one—forward weights operates recognition and backward weights implements imagination. It is perhaps the first network to do both. Helmholtz did not work in machine learning but he inspired the view of \"statistical inference engine whose function is to infer probable causes of sensory input\" (3). the stochastic binary neuron outputs a probability that its state is 0 or 1. The data input is normally not considered a layer, but in the Helmholtz machine generation mode, the data layer receives input from the middle layer has separate weights for this purpose, so it is considered a layer. Hence this network has 3 layers.\\n\\nVariational autoencoder\\nThese are inspired by Helmholtz machines and combines probability network with neural networks. An Autoencoder is a 3-layer CAM network, where the middle layer is supposed to be some internal representation of input patterns. The encoder neural network is a probability distribution qφ(z given x) and the decoder network is pθ(x given z). The weights are named phi & theta rather than W and V as in Helmholtz—a cosmetic difference. These 2 networks here can be fully connected, or use another NN scheme.\\n\\nComparison of networks\\nHebbian Learning, ART, SOM\\nThe classical example of unsupervised learning in the study of neural networks is Donald Hebb\\'s principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\\nAmong neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.\\n\\nProbabilistic methods\\nTwo of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\\nA central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\\n\\nApproaches\\nSome of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\\n\\nClustering methods include: hierarchical clustering, k-means, mixture models, DBSCAN, and OPTICS algorithm\\nAnomaly detection methods include: Local Outlier Factor, and Isolation Forest\\nApproaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques (Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition)\\n\\nMethod of moments\\nOne of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\\nIn particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.\\n\\nSee also\\nAutomated machine learning\\nCluster analysis\\nAnomaly detection\\nExpectation–maximization algorithm\\nGenerative topographic map\\nMeta-learning (computer science)\\nMultivariate analysis\\nRadial basis function network\\nWeak supervision\\n\\nReferences\\n\\n\\n== Further reading =='], ['Data science is an interdisciplinary academic field  that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured, and unstructured data.Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" in order to \"understand and analyse actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.A data scientist is the professional who creates programming code and combines it with statistical knowledge to create insights from data.\\n\\nFoundations\\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.\\n\\nRelationship to statistics\\nMany statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g. from images, text, sensors, transactions or customer information, etc) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a nonessential part of data science.Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.\\n\\nEtymology\\nEarly usage\\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name for computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".\\n\\nModern usage\\nIn 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catch-phrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating \"the job is more in demand than ever with employers\".The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association\\'s Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.There is still no consensus on the definition of data science, and it is considered by some to be a buzzword. Big data is a related marketing term. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.\\n\\nSee also\\nOpen Data Science Conference\\nScientific Data\\nWomen in Data\\nPython (programming language)\\nData engineering\\nBig data\\nMachine learning\\n\\n\\n== References ==', 'Data analysis  is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today\\'s business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.Data mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.\\n\\nThe process of data analysis\\nAnalysis, refers to dividing a whole into its separate components for individual examination. Data analysis, is a process for obtaining raw data, and subsequently converting it into information useful for decision-making by users. Data, is collected and analyzed to answer questions, test hypotheses, or disprove theories.\\nStatistician John Tukey, defined data analysis in 1961, as:\"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases. The CRISP framework, used in data mining, has similar steps.\\n\\nData requirements\\nThe data is necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analytics (or customers, who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained.  Data may be numerical or categorical (i.e., a text label for numbers).\\n\\nData collection\\nData is collected from a variety of sources. A list of data sources are available for study & research. The requirements may be communicated by analysts to custodians of the data; such as, Information Technology personnel within an organization. The data may also be collected from sensors in the environment, including traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.\\n\\nData processing\\nData, when initially obtained, must be processed or organized for analysis. For instance, these may involve placing data into rows and columns in a table format (known as structured data) for further analysis, often through the use of spreadsheet or statistical software.\\n\\nData cleaning\\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that the datum are entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example; with financial information, the totals for particular variables may be compared against separately published numbers that are believed to be reliable. Unusual amounts, above or below predetermined thresholds, may also be reviewed.  There are several types of data cleaning, that are dependent upon the type of data in the set; this could be phone numbers, email addresses, employers, or other values. Quantitative data methods for outlier detection, can be used to get rid of data that appears to have a higher likelihood of being input incorrectly. Textual data spell checkers can be used to lessen the amount of mis-typed words. However, it is harder to tell if the words themselves are correct.\\n\\nExploratory data analysis\\nOnce the datasets are cleaned, they can then be analyzed. Analysts may apply a variety of techniques, referred to as exploratory data analysis, to begin understanding the messages contained within the obtained data. The process of data exploration may result in additional data cleaning or additional requests for data; thus, the initialization of the iterative phases mentioned in the lead paragraph of this section. Descriptive statistics, such as, the average or median, can be generated to aid in understanding the data. Data visualization is also a technique used, in which the analyst is able to examine the data in a graphical format in order to obtain additional insights, regarding the messages within the data.\\n\\nModeling and algorithms\\nMathematical formulas or models (also known as algorithms), may be applied to the data in order to identify relationships among the variables; for example, using correlation or causation. In general terms, models may be developed to evaluate a specific variable based on other variable(s) contained within the dataset, with some residual error depending on the implemented model\\'s accuracy (e.g., Data = Model + Error).Inferential statistics, includes utilizing techniques that measure the relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X), provides an explanation for the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as (Y = aX + b + error), where the model is designed such that (a) and (b) minimize the error when the model predicts Y for a given range of values of X. Analysts may also attempt to build models that are descriptive of the data, in an aim to simplify analysis and communicate results.\\n\\nData product\\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. For instance, an application that analyzes data about customer purchase history, and uses the results to recommend other purchases the customer might enjoy.\\n\\nCommunication\\nOnce data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.When determining how to communicate the results, the analyst may consider implementing a variety of data visualization techniques to help communicate the message more clearly and efficiently to the audience. Data visualization uses information displays (graphics such as, tables and charts) to help communicate key messages contained in the data. Tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers; while charts (e.g., bar charts or line charts), may help explain the quantitative messages contained in the data.\\n\\nQuantitative messages\\nStephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\\nTime-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.\\nRanking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by salespersons (the category, with each salesperson a categorical subdivision) during a single period.  A bar chart may be used to show the comparison across the salespersons.\\nPart-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%).  A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.\\nDeviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period.  A bar chart can show the comparison of the actual versus the reference amount.\\nFrequency distribution: Shows the number of observations of a particular variable for a given interval, such as the number of years in which the stock market return is between intervals such as 0–10%, 11–20%, etc. A histogram, a type of bar chart, may be used for this analysis.\\nCorrelation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.\\nNominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.\\nGeographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.\\n\\nTechniques for analyzing quantitative data\\nAuthor Jonathan Koomey has recommended a series of best practices for understanding quantitative data.  These include:\\n\\nCheck raw data for anomalies prior to performing an analysis;\\nRe-perform important calculations, such as verifying columns of data that are formula driven;\\nConfirm main totals are the sum of subtotals;\\nCheck relationships between numbers that should be related in a predictable way, such as ratios over time;\\nNormalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;\\nBreak problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.For the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.\\n The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as \"Mutually Exclusive and Collectively Exhaustive\" or MECE.  For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as the revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).Analysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.Regression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.Necessary condition analysis (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X\\'s can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.\\n\\nAnalytical activities of data users\\nUsers may have particular data points of interest within a data set, as opposed to the general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.\\n\\nBarriers to effective analysis\\nBarriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.\\n\\nConfusing fact and opinion\\nEffective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011–2020 time period would add approximately $3.3 trillion to the national debt. Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.As another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects\". This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\\n\\nCognitive biases\\nThere are a variety of cognitive biases that can adversely affect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one\\'s preconceptions. In addition, individuals may discredit information that does not support their views.Analysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.\\n\\nInnumeracy\\nEffective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate.  Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.For example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.Analysts may also analyze data under different assumptions or scenario. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock.  Similarly, the CBO analyzes the effects of various policy options on the government\\'s revenue, outlays and deficits, creating alternative future scenarios for key measures.\\n\\nOther topics\\nSmart buildings\\nA data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.\\n\\nAnalytics and business intelligence\\nAnalytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that uses data to understand and analyze business performance to drive decision-making .\\n\\nEducation\\nIn education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators’ data analyses.\\n\\nPractitioner notes\\nThis section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.\\n\\nInitial data analysis\\nThe most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:\\n\\nQuality of data\\nThe quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms), normal imputation is needed.\\nAnalysis of extreme observations: outlying observations in the data are analyzed to see if they seem to disturb the distribution.\\nComparison and correction of differences in coding schemes: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.\\nTest for common-method variance.The choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.\\n\\nQuality of measurements\\nThe quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.\\nThere are two ways to assess measurement quality:\\n\\nConfirmatory factor analysis\\nAnalysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach\\'s α of the scales, and the change in the Cronbach\\'s alpha when an item would be deleted from a scale\\n\\nInitial transformations\\nAfter assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.\\nPossible transformations of variables are:\\nSquare root transformation (if the distribution differs moderately from normal)\\nLog-transformation (if the distribution differs substantially from normal)\\nInverse transformation (if the distribution differs severely from normal)\\nMake categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)\\n\\nDid the implementation of the study fulfill the intentions of the research design?\\nOne should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.Other possible data distortions that should be checked are:\\n\\ndropout (this should be identified during the initial data analysis phase)\\nItem non-response (whether this is random or not should be assessed during the initial data analysis phase)\\nTreatment quality (using manipulation checks).\\n\\nCharacteristics of data sample\\nIn any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.The characteristics of the data sample can be assessed by looking at:\\n\\nBasic statistics of important variables\\nScatter plots\\nCorrelations and associations\\nCross-tabulations\\n\\nFinal stage of the initial data analysis\\nDuring the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:\\n\\nIn the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?\\nIn the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?\\nIn the case of outliers: should one use robust analysis techniques?\\nIn case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?\\nIn the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?\\nIn case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?\\n\\nAnalysis\\nSeveral analyses can be used during the initial data analysis phase:\\nUnivariate statistics (single variable)\\nBivariate associations (correlations)\\nGraphical techniques (scatter plots)It is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:\\nNominal and ordinal variables\\nFrequency counts (numbers and percentages)\\nAssociations\\ncircumambulations (crosstabulations)\\nhierarchical loglinear analysis (restricted to a maximum of 8 variables)\\nloglinear analysis (to identify relevant/important variables and possible confounders)\\nExact tests or bootstrapping (in case subgroups are small)\\nComputation of new variables\\nContinuous variables\\nDistribution\\nStatistics (M, SD, variance, skewness, kurtosis)\\nStem-and-leaf displays\\nBox plots\\n\\nNonlinear analysis\\nNonlinear analysis is often necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods.  Nonlinear data analysis is closely related to nonlinear system identification.\\n\\nMain data analysis\\nIn the main analysis phase, analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.\\n\\nExploratory and confirmatory approaches\\nIn the main analysis phase, either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.Exploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.\\n\\nStability of results\\nIt is important to obtain some indication about how generalizable the results are. While this is often difficult to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing that.\\nCross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.\\nSensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.\\n\\nFree software for data analysis\\nNotable free software for data analysis include:\\n\\nDevInfo – A database system endorsed by the United Nations Development Group for monitoring and analyzing human development.\\nELKI – Data mining framework in Java with data mining oriented visualization functions.\\nKNIME – The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\\nOrange – A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.\\nPandas – Python library for data analysis.\\nPAW – FORTRAN/C data analysis framework developed at CERN.\\nR – A programming language and software environment for statistical computing and graphics.\\nROOT –  C++ data analysis framework developed at CERN.\\nSciPy – Python library for data analysis.\\nJulia - A programming language well-suited for numerical analysis and computational science.\\n\\nInternational data analysis contests\\nDifferent companies or organizations hold data analysis contests to encourage researchers to utilize their data or to solve a particular question using data analysis. A few examples of well-known international data analysis contests are as follows:\\nKaggle competition, which is held by Kaggle.\\nLTPP data analysis contest held by FHWA and ASCE.\\n\\nSee also\\nReferences\\nCitations\\nBibliography\\nAdèr, Herman J. (2008a). \"Chapter 14: Phases and initial steps in data analysis\".  In Adèr, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant\\'s companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 333–356. ISBN 9789079418015. OCLC 905799857.\\nAdèr, Herman J. (2008b). \"Chapter 15: The main analysis phase\".  In Adèr, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant\\'s companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 357–386. ISBN 9789079418015. OCLC 905799857.\\nTabachnick, B.G. & Fidell, L.S. (2007). Chapter 4: Cleaning up your act. Screening data prior to analysis. In B.G. Tabachnick & L.S. Fidell (Eds.), Using Multivariate Statistics, Fifth Edition (pp. 60–116). Boston: Pearson Education, Inc. / Allyn and Bacon.\\n\\nFurther reading\\n\\nAdèr, H.J. & Mellenbergh, G.J. (with contributions by D.J. Hand) (2008). Advising on Research Methods: A Consultant\\'s Companion. Huizen, the Netherlands: Johannes van Kessel Publishing.  ISBN 978-90-79418-01-5\\nChambers, John M.; Cleveland, William S.; Kleiner, Beat; Tukey, Paul A. (1983). Graphical Methods for Data Analysis, Wadsworth/Duxbury Press. ISBN 0-534-98052-X\\nFandango, Armando (2017). Python Data Analysis, 2nd Edition. Packt Publishers. ISBN 978-1787127487\\nJuran, Joseph M.; Godfrey, A. Blanton (1999). Juran\\'s Quality Handbook, 5th Edition. New York: McGraw Hill. ISBN 0-07-034003-X\\nLewis-Beck, Michael S. (1995). Data Analysis: an Introduction, Sage Publications Inc, ISBN 0-8039-5772-6\\nNIST/SEMATECH (2008) Handbook of Statistical Methods,\\nPyzdek, T, (2003). Quality Engineering Handbook, ISBN 0-8247-4614-7\\nRichard Veryard (1984). Pragmatic Data Analysis. Oxford : Blackwell Scientific Publications. ISBN 0-632-01311-7\\nTabachnick, B.G.; Fidell, L.S. (2007). Using Multivariate Statistics, 5th Edition. Boston: Pearson Education, Inc. / Allyn and Bacon, ISBN 978-0-205-45938-4', 'Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.\\nThe actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps.\\nThe difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.\\n\\nEtymology\\nIn the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983. Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative).\\nThe term data mining appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, a phrase \"database mining\"™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation; researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities. Currently, the terms data mining and knowledge discovery are used interchangeably.\\n\\nBackground\\nThe manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes\\' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.\\n\\nProcess\\nThe knowledge discovery in databases (KDD) process is commonly defined with the stages:\\n\\nSelection\\nPre-processing\\nTransformation\\nData mining\\nInterpretation/evaluation.It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:\\n\\nBusiness understanding\\nData understanding\\nData preparation\\nModeling\\nEvaluation\\nDeploymentor a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.\\nPolls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.\\n\\nPre-processing\\nBefore data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.\\n\\nData mining\\nData mining involves six common classes of tasks:\\nAnomaly detection (outlier/change/deviation detection) – The identification of unusual data records, that might be interesting or data errors that require further investigation.\\nAssociation rule learning (dependency modeling) – Searches for relationships between variables. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis.\\nClustering – is the task of discovering groups and structures in the data that are in some way or another \"similar\", without using known structures in the data.\\nClassification – is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as \"legitimate\" or as \"spam\".\\nRegression – attempts to find a function that models the data with the least error that is, for estimating the relationships among data or datasets.\\nSummarization – providing a more compact representation of the data set, including visualization and report generation.\\n\\nResults validation\\nData mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.\\nIf the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\\n\\nResearch\\nThe premier professional body in the field is the Association for Computing Machinery\\'s (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD). Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".Computer science conferences on data mining include:\\n\\nCIKM Conference – ACM Conference on Information and Knowledge Management\\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases\\nKDD Conference – ACM SIGKDD Conference on Knowledge Discovery and Data MiningData mining topics are also present in many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases.\\n\\nStandards\\nThere have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.\\nFor exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.\\n\\nNotable uses\\nData mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.\\n\\nPrivacy concerns and ethics\\nWhile the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to user behavior (ethical and otherwise).The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics. In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual\\'s privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.It is recommended to be aware of the following before data are collected:\\nThe purpose of the data collection and any (known) data mining projects.\\nHow the data will be used.\\nWho will be able to mine the data and use the data and their derivatives.\\nThe status of security surrounding access to the data.\\nHow collected data can be updated.Data may also be modified so as to become anonymous, so that individuals may not readily be identified. However, even \"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,\\nemotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling\\nprescription information to data mining companies who in turn provided the data\\nto pharmaceutical companies.\\n\\nSituation in Europe\\nEurope has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden\\'s global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.\\n\\nSituation in the United States\\nIn the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, \"\\'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,\\' says the AAHC. More importantly, the rule\\'s goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\" This underscores the necessity for data anonymity in data aggregation and mining practices.\\nU.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.\\n\\nCopyright law\\nSituation in Europe\\nUnder European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist, so data mining becomes subject to intellectual property owners\\' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception. The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions.\\nSince 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020.The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.\\n\\nSituation in the United States\\nUS copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google\\'s digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.\\n\\nSoftware\\nFree open-source data mining software and applications\\nThe following applications are available under free/open-source licenses. Public access to application source code is also available.\\n\\nCarrot2: Text and search results clustering framework.\\nChemicalize.org: A chemical structure miner and web search engine.\\nELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language.\\nGATE: a natural language processing and language engineering tool.\\nKNIME: The Konstanz Information Miner, a user-friendly and comprehensive data analytics framework.\\nMassive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language.\\nMEPX: cross-platform tool for regression and classification problems based on a Genetic Programming variant.\\nmlpack: a collection of ready-to-use machine learning algorithms written in the C++ language.\\nNLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language.\\nOpenNN: Open neural networks library.\\nOrange: A component-based data mining and machine learning software suite written in the Python language.\\nPSPP: Data mining and statistics software under the GNU Project similar to SPSS\\nR: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project.\\nscikit-learn: An open-source machine learning library for the Python programming language;\\nTorch: An open-source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms.\\nUIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video – originally developed by IBM.\\nWeka: A suite of machine learning software applications written in the Java programming language.\\n\\nProprietary data-mining software and applications\\nThe following applications are available under proprietary licenses.\\n\\nAngoss KnowledgeSTUDIO: data mining tool\\nLIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach.\\nPolyAnalyst: data and text mining software by Megaputer Intelligence.\\nMicrosoft Analysis Services: data mining software provided by Microsoft.\\nNetOwl: suite of multilingual text and entity analytics products that enable data mining.\\nOracle Data Mining: data mining software by Oracle Corporation.\\nPSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE.\\nQlucore Omics Explorer: data mining software.\\nRapidMiner: An environment for machine learning and data mining experiments.\\nSAS Enterprise Miner: data mining software provided by the SAS Institute.\\nSPSS Modeler: data mining software provided by IBM.\\nSTATISTICA Data Miner: data mining software provided by StatSoft.\\nTanagra: Visualisation-oriented data mining software, also for teaching.\\nVertica: data mining software provided by Hewlett-Packard.\\nGoogle Cloud Platform: automated custom ML models managed by Google.\\nAmazon SageMaker: managed service provided by Amazon for creating & productionising custom ML models.\\n\\nSee also\\nMethods\\nApplication domains\\nApplication examples\\n\\nRelated topicsFor more information about extracting information out of data (as opposed to analyzing data), see:\\n\\nOther resourcesInternational Journal of Data Warehousing and Mining\\n\\nReferences\\nFurther reading\\nExternal links\\n\\nKnowledge Discovery Software at Curlie\\nData Mining Tool Vendors at Curlie', 'Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly because of a lack of formal definition, the interpretation that seems to best describe big data is the one associated with large body of information that we could not comprehend when used only in smaller amounts.Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, then the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"\\n\\nDefinition\\nThe term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.\\nBig data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time. Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data. Big data \"size\" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.\\nBig data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.\"Variety\", \"veracity\", and various other \"Vs\" are added by some organizations to describe it, a revision challenged by some industry authorities. The Vs of big data were often referred to as the \"three Vs\", \"four Vs\", and \"five Vs\". They represented the qualities of big data in volume, variety, velocity, veracity, and value. Variability is often included as an additional quality of big data.\\nA 2018 definition states \"Big data is where parallel computing tools are needed to handle data\", and notes, \"This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd\\'s relational model.\"In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.\\n\\nBig data vs. business intelligence\\nThe growing maturity of the concept more starkly delineates the difference between \"big data\" and \"business intelligence\":\\nBusiness intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc.\\nBig data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.\\n\\nCharacteristics\\nBig data can be described by the following characteristics:\\n\\nVolume\\nThe quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes.Variety\\nThe type and nature of the data. The earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. The big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.Velocity\\nThe speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing.Veracity\\nThe truthfulness or reliability of the data, which refers to the data quality and the data value. Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it. The data quality of captured data can vary greatly, affecting an accurate analysis.Value\\nThe worth in information that can be achieved by the processing and analysis of large datasets. Value also can be measured by an assessment of the other qualities of big data. Value may also represent the profitability of information that is retrieved from the analysis of big data.Variability\\nThe characteristic of the changing formats, structure, or sources of big data. Big data can include structured, unstructured, or combinations of structured and unstructured data. Big data analysis may integrate raw data from multiple sources. The processing of raw data may also involve transformations of unstructured data to structured data.Other possible characteristics of big data are:\\nExhaustive\\nWhether the entire system (i.e., \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\textstyle n}\\n  =all) is captured or recorded or not. Big data may or may not include all the available data from sources.Fine-grained and uniquely lexical\\nRespectively, the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified.Relational\\nIf the data collected contains common fields that would enable a conjoining, or meta-analysis, of different data sets.Extensional\\nIf new fields in each element of the data collected can be added or changed easily.Scalability\\nIf the size of the big data storage system can expand rapidly.\\n\\nArchitecture\\nBig data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s. For many years, WinterCorp published the largest database report.Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added unstructured data types including XML, JSON, and Avro.\\nIn 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.\\nCERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current \"big data\" movement.\\nIn 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the \"map\" step). The results are then gathered and delivered (the \"reduce\" step). The framework was very successful, so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named \"Hadoop\". Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds in-memory processing and the ability to set up many operations (not just map followed by reducing).\\nMIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled \"Big Data Solution Offering\". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.\\n\\nTechnologies\\nA 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:\\nTechniques for analyzing data, such as A/B testing, machine learning, and natural language processing\\nBig data technologies, like business intelligence, cloud computing, and databases\\nVisualization, such as charts, graphs, and other displays of the dataMultidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.\\nAdditional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.DARPA\\'s Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called \"Ayasdi\".The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures—storage area network (SAN) and network-attached storage (NAS)— is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.\\nReal or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good—data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.\\n\\nApplications\\nBig data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year, about twice as fast as the software business as a whole.Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world\\'s effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data, which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).\\nWhile many vendors offer off-the-shelf products for big data, experts promote the development of in-house custom-tailored systems if the company has sufficient technical capabilities.\\n\\nGovernment\\nThe use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation, but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.\\nCivil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.\\n\\nInternational development\\nResearch on the effective usage of information and communication technologies for development (also known as \"ICT4D\") suggests that big data technology can make important contributions but also present unique challenges to international development. Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management. Additionally, user-generated data offers new opportunities to give the unheard a voice. However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues. The challenge of \"big data for development\" is currently evolving toward the application of this data through machine learning, known as \"artificial intelligence for development (AI4D).\\n\\nBenefits\\nA major practical application of big data for development has been \"fighting poverty with data\". In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata  and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty. Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues  argue that digital trace data has several benefits such as:\\n\\nThematic coverage: including areas that were previously difficult or impossible to measure\\nGeographical coverage: our international sources provided sizable and comparable data for almost all countries, including many small countries that usually are not included in international inventories\\nLevel of detail: providing fine-grained data with many interrelated variables, and new aspects, like network connections\\nTimeliness and timeseries: graphs can be produced within days of being collected\\n\\nChallenges\\nAt the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:\\n\\nRepresentativeness. While traditional development statistics is mainly concerned with the representativeness of random survey samples, digital trace data is never a random sample.\\nGeneralizability. While observational data always represents this source very well, it only represents what it represents, and nothing more. While it is tempting to generalize from specific observations of one platform to broader settings, this is often very deceptive.\\nHarmonization. Digital trace data still requires international harmonization of indicators. It adds the challenge of so-called \"data-fusion\", the harmonization of different sources.\\nData overload. Analysts and institutions are not used to effectively deal with a large number of variables, which is efficiently done with interactive dashboards. Practitioners still lack a standard workflow that would allow researchers, users and policymakers to efficiently and effectively.\\n\\nFinance\\nBig Data is being rapidly adopted in Finance to 1) speed up processing and 2) deliver better, more informed inferences, both internally and to the clients of the financial institutions.. The financial applications of Big Data range from investing decisions and trading (processing volumes of available price data, limit order books, economic data and more, all at the same time), portfolio management (optimizing over an increasingly large array of financial instruments, potentially selected from different asset classes), risk management (credit rating based on extended information), and any other aspect where the data inputs are large.\\n\\nHealthcare\\nBig data analytics was used in healthcare by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries. Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. \"Big data very often means \\'dirty data\\' and the fraction of data inaccuracies increases with data volume growth.\" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.Big data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research. Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.\\nA related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.\\n For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily.\\n Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.\\n\\nThese are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.\\n\\nEducation\\nA McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannan is that marketing has several sub domains (e.g., advertising, promotions,\\nproduct development, branding) that all use different types of data.\\n\\nMedia\\nTo understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer\\'s mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.\\nTargeting of consumers (for advertising by marketers)\\nData capture\\nData journalism: publishers and journalists use big data tools to provide unique and innovative insights and infographics.Channel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.\\n\\nInsurance\\nHealth insurance providers are collecting data on social \"determinants of health\" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.\\n\\nInternet of things (IoT)\\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts.\\nKevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\"\\n\\nInformation technology\\nEspecially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them. ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data.\\n\\nCase studies\\nGovernment\\nChina\\nThe Integrated Joint Operations Platform (IJOP, 一体化联合作战平台) is used by the government to monitor the population, particularly Uyghurs. Biometrics, including DNA samples, are gathered through a program of free physicals.\\nBy 2020, China plans to give all its citizens a personal \"social credit\" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology.\\n\\nIndia\\nBig data analysis was tried out for the BJP to win the 2014 Indian General Election.\\nThe Indian government uses numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.\\n\\nIsrael\\nPersonalized diabetic treatments can be created through GlucoMe\\'s big data solution.\\n\\nUnited Kingdom\\nExamples of uses of big data in public services:\\n\\nData on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify and examine the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.\\nJoining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as Meals on Wheels. The connection of data allowed the local authority to avoid any weather-related delay.\\n\\nUnited States\\nIn 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government. The initiative is composed of 84 different big data programs spread across six departments.\\nBig data analysis played a large role in Barack Obama\\'s successful 2012 re-election campaign.\\nThe United States Federal Government owns five of the ten most powerful supercomputers in the world.\\nThe Utah Data Center has been constructed by the United States National Security Agency. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabytes. This has posed security concerns regarding the anonymity of the data collected.\\n\\nRetail\\nWalmart handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US Library of Congress.\\nWindermere Real Estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.\\nFICO Card Detection System protects accounts worldwide.\\n\\nScience\\nThe Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.99995% of these streams, there are 1,000 collisions of interest per second.As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.\\nIf all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication. To put the number in perspective, this is equivalent to 500 quintillion (5×1020) bytes per day, almost 200 times more than all the other sources combined in the world.\\nThe Square Kilometre Array is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day. It is considered one of the most ambitious scientific projects ever undertaken.\\nWhen the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.\\nDecoding the human genome originally took 10 years to process; now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by Moore\\'s law.\\nThe NASA Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.\\nGoogle\\'s DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any \"friction points\", or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google\\'s search server to scale social experiments that would usually take years, instantly.\\n23andme\\'s DNA database contains the genetic information of over 1,000,000 people worldwide. The company explores selling the \"anonymous aggregated genetic data\" to other researchers and pharmaceutical companies for research purposes if patients give their consent. Ahmad Hariri, professor of psychology and neuroscience at Duke University who has been using 23andMe in his research since 2009 states that the most important aspect of the company\\'s new service is that it makes genetic research accessible and relatively cheap for scientists. A study that identified 15 genome sites linked to depression in 23andMe\\'s database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.\\nComputational fluid dynamics (CFD) and hydrodynamic turbulence research generate massive data sets. The Johns Hopkins Turbulence Databases (JHTDB) contains over 350 terabytes of spatiotemporal fields from Direct Numerical simulations of various turbulent flows. Such data have been difficult to share using traditional methods such as downloading flat simulation output files. The data within JHTDB can be accessed using \"virtual sensors\" with various access modes ranging from direct web-browser queries, access through Matlab, Python, Fortran and C programs executing on clients\\' platforms, to cut out services to download raw data. The data have been used in over 150 scientific publications.\\n\\nSports\\nBig data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.\\nFuture performance of players could be predicted as well. Thus, players\\' value and salary is determined by data collected throughout the season.In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.\\nBased on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.\\n\\nTechnology\\neBay.com uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.\\nAmazon.com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world\\'s three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.\\nFacebook handles 50 billion photos from its user base. As of June 2017, Facebook reached 2 billion monthly active users.\\nGoogle was handling roughly 100 billion searches per month as of August 2012.\\n\\nCOVID-19\\nDuring the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.Governments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.\\n\\nResearch activities\\nEncrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.In March 2012, The White House announced a national \"Big Data Initiative\" that consisted of six federal departments and agencies committing more than $200 million to big data research projects.The initiative included a National Science Foundation \"Expeditions in Computing\" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department\\'s Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department\\'s supercomputers.\\nThe U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.Computational social sciences – Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the \"future orientation index\". They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.\\nTobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data. Regarding big data, such concepts of magnitude are relative. As it is stated \"If the past is of any guidance, then today\\'s big data most likely will not be considered as such in the near future.\"\\n\\nSampling big data\\nA research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient. Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.\\nThere has been some work done in sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed.\\n\\nCritique\\nCritiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.\\n\\nCritiques of the big data paradigm\\n\"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data.\" In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson\\'s assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by \"big judgment\", according to an article in the Harvard Business Review.Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system\\'s dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggest to use \"abductive reasoning as a first step in the research process in order to bring context to consumers\\' digital traces and make new theories emerge\".\\nAdditionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (e.g. contingency tables) typically employed with smaller data sets.\\nIn health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.\\nA new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation. In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction (\"Glory of Science and Philosophy scandal\", C. D. Broad, 1926) are to be considered.Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.Nayef Al-Rodhan argues that a new kind of social contract will be needed to protect individual liberties in the context of big data and giant corporations that own vast amounts of information, and that the use of big data should be monitored and better regulated at the national and international levels. Barocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.\\n\\nCritiques of the \"V\" model\\nThe \"V\" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:\\nData completeness: understanding of the non-obvious from data\\nData correlation, causation, and predictability: causality as not essential requirement to achieve predictability\\nExplainability and interpretability: humans desire to understand and accept what they understand, where algorithms do not cope with this\\nLevel of automated decision-making: algorithms that support automated decision making and algorithmic self-learning\\n\\nCritiques of novelty\\nLarge data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM\\'s punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial \"big data\". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.\\n\\nCritiques of big data execution\\nUlf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a \"fad\" in scientific research. Researcher danah boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data. This approach may lead to results that have a bias in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.\\nIn the provocative article \"Critical Questions for Big Data\", the authors title big data a part of mythology: \"large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy\". Users of big data are often \"lost in the sheer volume of numbers\", and \"working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth\". Recent developments in BI domain, such as pro-active reporting especially target improvements in the usability of big data, through automated filtering of non-useful data and correlations. Big structures are full of spurious correlations either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers \"speak for themselves\" and revolutionize scientific method, is questioned. Catherine Tucker has pointed to \"hype\" around big data, writing \"By itself, big data is unlikely to be valuable.\" The article explains: \"The many contexts where data is cheap relative to the cost of retaining talent to process it, suggests that processing skills are more important than data itself in creating value for a firm.\"Big data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.Big data is a buzzword and a \"vague term\", but at the same time an \"obsession\" with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.\\nBig data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.\\nOn the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.\\nIoannidis argued that \"most published research findings are false\" due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a \"significant\" result being false grows fast – even more so, when only positive results are published.\\nFurthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. Presidential Election with varying degrees of success.\\n\\nCritiques of big data policing and surveillance\\nBig data has been used in policing and surveillance by institutions like law enforcement and corporations. Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne\\'s Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways:\\n\\nPlacing people under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm\\nIncreasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system\\nEncouraging members of society to abandon interactions with institutions that would create a digital trace, thus creating obstacles to social inclusionIf these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.\\n\\nSee also\\nReferences\\nFurther reading\\nPeter Kinnaird; Inbal Talgam-Cohen, eds. (2012). \"Big Data\". XRDS: Crossroads, The ACM Magazine for Students. Vol. 19, no. 1. Association for Computing Machinery. ISSN 1528-4980. OCLC 779657714.\\nJure Leskovec; Anand Rajaraman; Jeffrey D. Ullman (2014). Mining of massive datasets. Cambridge University Press. ISBN 9781107077232. OCLC 888463433.\\nViktor Mayer-Schönberger; Kenneth Cukier (2013). Big Data: A Revolution that Will Transform how We Live, Work, and Think. Houghton Mifflin Harcourt. ISBN 9781299903029. OCLC 828620988.\\nPress, Gil (9 May 2013). \"A Very Short History of Big Data\". forbes.com. Jersey City, NJ. Retrieved 17 September 2016.\\nStephens-Davidowitz, Seth (2017). Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are. Dey Street Books. ISBN 978-0062390851.\\n\"Big Data: The Management Revolution\". Harvard Business Review. October 2012.\\nO\\'Neil, Cathy (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books. ISBN 978-0553418835.\\n\\nExternal links\\n Media related to Big data at Wikimedia Commons\\n The dictionary definition of big data at Wiktionary', 'Data and information visualization (data viz or info viz) is the practice of designing and creating easy-to-communicate and easy-to-understand graphic or visual representation of a large amount of complex quantitative and qualitative data and information from a certain domain of expertise with the help of static, dynamic or interactive visual items for a broader audience to help them visually explore and discover, quickly understand, interpret and gain important insights into otherwise difficult-to-identify structures, relationships, correlations, local and global patterns, trends, variations, constancy, clusters, outliers and unusual groupings within data (exploratory visualization). When intended for the general public (mass communication) to convey a concise version of known, specific information in a clear and engaging manner (presentational or explanatory visualization), it is typically called information graphics.  \\nData visualization is concerned with visually presenting sets of primarily quantitative raw data in a schematic form. The visual formats used in data visualization include tables, charts and graphs (e.g. pie charts, bar charts, line charts, area charts, cone charts, pyramid charts, donut charts, histograms, spectrograms, cohort charts, waterfall charts, funnel charts, bullet graphs, etc.), diagrams, plots (e.g. scatter plots, distribution plots, box-and-whisker plots), geospatial maps (such as proportional symbol maps, choropleth maps, isopleth maps and heat maps), figures, correlation matrices, percentage gauges, etc., which sometimes can be combined in a dashboard. Information visualization, on the other hand, deals with multiple, large-scale and complicated datasets which contain quantitative (numerical) data as well as qualitative (non-numerical, i.e. verbal or graphical) and primarily abstract information and its goal is to add value to raw data, improve the viewers\\' comprehension, reinforce their cognition and help them derive insights and make decisions as they navigate and interact with the computer-supported graphical display. Visual tools used in information visualization include maps (such as tree maps), animations, infographics, Sankey diagrams, flow charts, network diagrams, semantic networks, entity-relationship diagrams, venn diagrams, timelines, mind maps, etc. Emerging technologies like virtual, augmented and mixed reality have the potential to make information visualization more immersive,  intuitive, interactive and easily manipulable and thus enhance the user\\'s visual perception and cognition. In data and information visualization, the goal is to graphically present and explore abstract, non-physical and non-spatial data collected from databases, information systems, file systems, documents, business information, financial data, etc. (presentational and exploratory visualization) which is different from the field of scientific visualization, where the goal is to render realistic images based on physical and spatial scientific data to confirm or reject hypotheses (confirmatory visualization).Effective data visualization is properly sourced, contextualized, simple and uncluttered. The underlying data is accurate and up-to-date to make sure that insights are reliable. Graphical items are well-chosen for the given datasets and aesthetically appealing, with shapes, colors and other visual elements used deliberately in a meaningful and non-distracting manner. The visuals are accompanied by supporting texts (labels and titles). These verbal and graphical components complement each other to ensure clear, quick and memorable understanding. Effective information visualization is aware of the needs and concerns and the level of expertise of the target audience, deliberately guiding them to the intended conclusion. Such effective visualization can be used not only for conveying specialized, complex, big data-driven ideas to a wider group of non-technical audience in a visually appealing, engaging and accessible manner, but also to domain experts and executives for making decisions, monitoring performance, generating new ideas and stimulating research. In addition, data scientists, data analysts and data mining specialists use data visualization to check the quality of data, find errors, unusual gaps and missing values in data, clean data, explore the structures and features of data and assess outputs of data-driven models. In business, data and information visualization can be part of data storytelling where it is paired with a narrative structure or storyline to contextualize the analyzed data and communicate the insights gained from analyzing the data clearly and memorably with the goal of convincing the audience into making a decision or taking an action in order to create business value. This can be contrasted with the field of statistical graphics, where complex statistical data are communicated graphically in an accurate and precise manner among researchers and analysts with statistical expertise to help them perform exploratory data analysis or to convey the results of such analyses, where visual appeal, capturing attention to a certain issue and storytelling are not as important.The field of data and information visualization is of interdisciplinary nature as it incorporates principles found in the disciplines of descriptive statistics (as early as the 18th century), visual communication, graphic design, cognitive science and, more recently, interactive computer graphics and human-computer interaction. Since effective visualization requires design skills, statistical skills and computing skills, it is argued by authors such as Gershon and Page that it is both an art and a science. The neighboring field of visual analytics marries statistical data analysis, data and information visualization and human analytical reasoning through interactive visual interfaces to help human users reach conclusions, gain actionable insights and make informed decisions which are otherwise difficult for computers to do.\\nResearch into how people read and misread various types of visualizations is helping to determine what types and features of visualizations are most understandable and effective in conveying information. On the other hand, unintentionally poor or intentionally misleading and deceptive visualizations can function as powerful tools which disseminate misinformation, manipulate public perception and divert public opinion toward a certain agenda.\\n\\nOverview\\nThe field of data and information visualization has emerged \"from research in human–computer interaction, computer science, graphics, visual design, psychology, and business methods. It is increasingly applied as a critical component in scientific research, digital libraries, data mining, financial data analysis, market studies, manufacturing production control, and drug discovery\".Data and information visualization presumes that \"visual representations and interaction techniques take advantage of the human eye\\'s broad bandwidth pathway into the mind to allow users to see, explore, and understand large amounts of information at once. Information visualization focused on the creation of approaches for conveying abstract information in intuitive ways.\"Data analysis is an indispensable part of all applied research and problem solving in industry. The most fundamental data analysis approaches are visualization (histograms, scatter plots, surface plots, tree maps, parallel coordinate plots, etc.), statistics (hypothesis test, regression, PCA, etc.), data mining (association mining, etc.), and machine learning methods (clustering, classification, decision trees, etc.). Among these approaches,  information visualization, or visual data analysis, is the most reliant on the cognitive skills of human analysts, and allows the discovery of unstructured actionable insights that are limited only by human imagination and creativity. The analyst does not have to learn any sophisticated methods to be able to interpret the visualizations of the data. Information visualization is also a hypothesis generation scheme, which can be, and is typically followed by more analytical or formal analysis, such as statistical hypothesis testing.\\nTo communicate information clearly and efficiently, data visualization uses statistical graphics, plots, information graphics and other tools. Numerical data may be encoded using dots, lines, or bars, to visually communicate a quantitative message. Effective visualization helps users analyze and reason about data and evidence. It makes complex data more accessible, understandable, and usable, but can also be reductive. Users may have particular analytical tasks, such as making comparisons or understanding causality, and the design principle of the graphic (i.e., showing comparisons or showing causality) follows the task. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.\\nData visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e.g., points, lines, or bars) contained in graphics. The goal is to communicate information clearly and efficiently to users. It is one of the steps in data analysis or data science. According to Vitaly Friedman (2008) the \"main goal of data visualization is to communicate information clearly and effectively through graphical means. It doesn\\'t mean that data visualization needs to look boring to be functional or extremely sophisticated to look beautiful. To convey ideas effectively, both aesthetic form and functionality need to go hand in hand, providing insights into a rather sparse and complex data set by communicating its key aspects in a more intuitive way. Yet designers often fail to achieve a balance between form and function, creating gorgeous data visualizations which fail to serve their main purpose — to communicate information\".Indeed, Fernanda Viegas and Martin M. Wattenberg suggested that an ideal visualization should not only communicate clearly, but stimulate viewer engagement and attention.Data visualization is closely related to information graphics, information visualization, scientific visualization, exploratory data analysis and statistical graphics. In the new millennium, data visualization has become an active area of research, teaching and development. According to Post et al. (2002), it has united scientific and information visualization.In the commercial environment data visualization is often referred to as dashboards. Infographics are another very common form of data visualization.\\n\\nPrinciples\\nCharacteristics of effective graphical displays\\nEdward Tufte has explained that users of information displays are executing particular analytical tasks  such as making comparisons. The design principle of the information graphic should support the analytical task. As William Cleveland and Robert McGill show, different graphical elements accomplish this more or less effectively. For example, dot plots and bar charts outperform pie charts.In his 1983 book The Visual Display of Quantitative Information, Edward Tufte defines \\'graphical displays\\' and principles for effective graphical display in the following passage:\\n\"Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. Graphical displays should:\\n\\nshow the data\\ninduce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production, or something else\\navoid distorting what the data has to say\\npresent many numbers in a small space\\nmake large data sets coherent\\nencourage the eye to compare different pieces of data\\nreveal the data at several levels of detail, from a broad overview to the fine structure\\nserve a reasonably clear purpose: description, exploration, tabulation, or decoration\\nbe closely integrated with the statistical and verbal descriptions of a data set.Graphics reveal data.  Indeed graphics can be more precise and revealing than conventional statistical computations.\"For example, the Minard diagram shows the losses suffered by Napoleon\\'s army in the 1812–1813 period. Six variables are plotted: the size of the army, its location on a two-dimensional surface (x and y), time, the direction of movement, and temperature. The line width illustrates a comparison (size of the army at points in time), while the temperature axis suggests a cause of the change in army size. This multivariate display on a two-dimensional surface tells a story that can be grasped immediately while identifying the source data to build credibility.  Tufte wrote in 1983 that: \"It may well be the best statistical graphic ever drawn.\"Not applying these principles may result in misleading graphs, distorting the message, or supporting an erroneous conclusion. According to Tufte, chartjunk refers to the extraneous interior decoration of the graphic that does not enhance the message or gratuitous three-dimensional or perspective effects. Needlessly separating the explanatory key from the image itself, requiring the eye to travel back and forth from the image to the key, is a form of \"administrative debris.\" The ratio of \"data to ink\" should be maximized, erasing non-data ink where feasible.The Congressional Budget Office summarized several best practices for graphical displays in a June 2014 presentation.  These included: a) Knowing your audience; b) Designing graphics that can stand alone outside the report\\'s context; and c) Designing graphics that communicate the key messages in the report.\\n\\nQuantitative messages\\nAuthor Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message:\\n\\nTime-series: A single variable is captured over a period of time, such as the unemployment rate or temperature measures over a 10-year period. A line chart may be used to demonstrate the trend over time.\\nRanking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period.  A bar chart may be used to show the comparison across the sales persons.\\nPart-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%).  A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.\\nDeviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period.  A bar chart can show comparison of the actual versus the reference amount.\\nFrequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0–10%, 11–20%, etc. A histogram, a type of bar chart, may be used for this analysis. A boxplot helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc.\\nCorrelation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.\\nNominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.\\nGeographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.Analysts reviewing a set of data may consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis.\\n\\nVisual perception and data visualization\\nA human can distinguish differences in line length, shape, orientation, distances, and color (hue) readily without significant processing effort; these are referred to as \"pre-attentive attributes\".  For example, it may require significant time and effort (\"attentive processing\") to identify the number of times the digit \"5\" appears in a series of numbers; but if that digit is different in size, orientation, or color, instances of the digit can be noted quickly through pre-attentive processing.Compelling graphics take advantage of pre-attentive processing and attributes and the relative strength of these attributes. For example, since humans can more easily process differences in line length than surface area, it may be more effective to use a bar chart (which takes advantage of line length to show comparison) rather than pie charts (which use surface area to show comparison).\\n\\nHuman perception/cognition and data visualization\\nAlmost all data visualizations are created for human consumption. Knowledge of human perception and cognition is necessary when designing intuitive visualizations. Cognition refers to processes in human beings like perception, attention, learning, memory, thought, concept formation, reading, and problem solving. Human visual processing is efficient in detecting changes and making comparisons between quantities, sizes, shapes and variations in lightness. When properties of symbolic data are mapped to visual properties, humans can browse through large amounts of data efficiently. It is estimated that 2/3 of the brain\\'s neurons can be involved in visual processing. Proper visualization provides a different approach to show potential connections, relationships, etc. which are not as obvious in non-visualized quantitative data. Visualization can become a means of data exploration.\\nStudies have shown individuals used on average 19% less cognitive resources, and 4.5% better able to recall details when comparing data visualization with text.\\n\\nHistory\\nThe modern study of visualization started with computer graphics, which \"has from its beginning been used to study scientific problems. However, in its early days the lack of graphics power often limited its usefulness. The recent emphasis on visualization started in 1987 with the special issue of Computer Graphics on Visualization in Scientific Computing. Since then there have been several conferences and workshops, co-sponsored by the IEEE Computer Society and ACM SIGGRAPH\". They have been devoted to the general topics of data visualization, information visualization and scientific visualization, and more specific areas such as volume visualization.\\nIn 1786, William Playfair published the first presentation graphics.\\n\\nThere is no comprehensive \\'history\\' of data visualization. There are no accounts that span the entire development of visual thinking and the visual representation of data, and which collate the contributions of disparate disciplines. Michael Friendly and Daniel J Denis of York University are engaged in a project that attempts to provide a comprehensive history of visualization. Contrary to general belief, data visualization is not a modern development. Since prehistory, stellar data, or information such as location of stars were visualized on the walls of caves (such as those found in Lascaux Cave in Southern France) since the Pleistocene era. Physical artefacts such as Mesopotamian clay tokens (5500 BC), Inca quipus (2600 BC) and Marshall Islands stick charts (n.d.) can also be considered as visualizing quantitative information.The first documented data visualization can be tracked back to 1160 B.C. with Turin Papyrus Map which accurately illustrates the distribution of geological resources and provides information about quarrying of those resources. Such maps can be categorized as thematic cartography, which is a type of data visualization that presents and communicates specific data and information through a geographical illustration designed to show a particular theme connected with a specific geographic area. Earliest documented forms of data visualization were various thematic maps from different cultures and ideograms and hieroglyphs that provided and allowed interpretation of information illustrated. For example, Linear B tablets of Mycenae provided a visualization of information regarding Late Bronze Age era trades in the Mediterranean. The idea of coordinates was used by ancient Egyptian surveyors in laying out towns, earthly and heavenly positions were located by something akin to latitude and longitude at least by 200 BC, and the map projection of a spherical earth into latitude and longitude by Claudius Ptolemy [c.\\u200985–c.\\u2009165] in Alexandria would serve as reference standards until the 14th century.The invention of paper and parchment allowed further development of visualizations throughout history. Figure shows a graph from the 10th or possibly 11th century that is intended to be an illustration of the planetary movement, used in an appendix of a textbook in monastery schools. The graph apparently was meant to represent a plot of the inclinations of the planetary orbits as a function of the time. For this purpose, the zone of the zodiac was represented on a plane with a horizontal line divided into thirty parts as the time or longitudinal axis. The vertical axis designates the width of the zodiac. The horizontal scale appears to have been chosen for each planet individually for the periods cannot be reconciled. The accompanying text refers only to the amplitudes. The curves are apparently not related in time.\\n\\nBy the 16th century, techniques and instruments for precise observation and measurement of physical quantities, and geographic and celestial position were well-developed (for example, a \"wall quadrant\" constructed by Tycho Brahe [1546–1601], covering an entire wall in his observatory). Particularly important were the development of triangulation and other methods to determine mapping locations accurately. Very early, the measure of time led scholars to develop innovative way of visualizing the data (e.g. Lorenz Codomann in 1596, Johannes Temporarius in 1596).\\n\\nFrench philosopher and mathematician René Descartes and Pierre de Fermat developed analytic geometry and two-dimensional coordinate system which heavily influenced the practical methods of displaying and calculating values. Fermat and Blaise Pascal\\'s work on statistics and probability theory laid the groundwork for what we now conceptualize as data. According to the Interaction Design Foundation, these developments allowed and helped William Playfair, who saw potential for graphical communication of quantitative data, to generate and develop graphical methods of statistics.  In the second half of the 20th century, Jacques Bertin used quantitative graphs to represent information \"intuitively, clearly, accurately, and efficiently\".John Tukey and Edward Tufte pushed the bounds of data visualization; Tukey with his new statistical approach of exploratory data analysis and Tufte with his book \"The Visual Display of Quantitative Information\" paved the way for refining data visualization techniques for more than statisticians. With the progression of technology came the progression of data visualization; starting with hand-drawn visualizations and evolving into more technical applications – including interactive designs leading to software visualization.Programs like SAS, SOFA, R, Minitab, Cornerstone and more allow for data visualization in the field of statistics. Other data visualization applications, more focused and unique to individuals, programming languages such as D3, Python and JavaScript help to make the visualization of quantitative data a possibility.  Private schools have also developed programs to meet the demand for learning data visualization and associated programming libraries, including free programs like The Data Incubator or paid programs like General Assembly.Beginning with the symposium \"Data to Discovery\" in 2013, ArtCenter College of Design, Caltech and JPL in Pasadena have run an annual program on interactive data visualization. The program asks: How can interactive data visualization help scientists and engineers explore their data more effectively? How can computing, design, and design thinking help maximize research results? What methodologies are most effective for leveraging knowledge from these fields? By encoding relational information with appropriate visual and interactive characteristics to help interrogate, and ultimately gain new insight into data, the program develops new interdisciplinary approaches to complex science problems, combining design thinking and the latest methods from computing, user-centered design, interaction design and 3D graphics.\\n\\nTerminology\\nData visualization involves specific terminology, some of which is derived from statistics. For example, author Stephen Few defines two types of data, which are used in combination to support a meaningful analysis or visualization:\\n\\nCategorical: Represent groups of objects with a particular characteristic. Categorical variables can either be nominal or ordinal. Nominal variables for example gender have no order between them and are thus nominal. Ordinal variables are categories with an order, for sample recording the age group someone falls into.\\nQuantitative: Represent measurements, such as the height of a person or the temperature of an environment. Quantitative variables can either be continuous or discrete. Continuous variables capture the idea that measurements can always be made more precisely. While discrete variables have only a finite number of possibilities, such as a count of some outcomes or an age measured in whole years.The distinction between quantitative and categorical variables is important because the two types require different methods of visualization.\\nTwo primary types of information displays are tables and graphs.\\n\\nA table contains quantitative data organized into rows and columns with categorical labels. It is primarily used to look up specific values. In the example above, the table might have categorical column labels representing the name (a qualitative variable) and age (a quantitative variable), with each row of data representing one person (the sampled experimental unit or category subdivision).\\nA graph is primarily used to show relationships among data and portrays values encoded as visual objects (e.g., lines, bars, or points). Numerical values are displayed within an area delineated by one or more axes. These axes provide scales (quantitative and categorical) used to label and assign values to the visual objects. Many graphs are also referred to as charts.Eppler and Lengler have developed the \"Periodic Table of Visualization Methods,\" an interactive chart displaying various data visualization methods. It includes six types of data visualization methods: data, information, concept, strategy, metaphor and compound.\\n\\nTechniques\\nOther techniques\\nCartogram\\nCladogram (phylogeny)\\nConcept Mapping\\nDendrogram (classification)\\nInformation visualization reference model\\nGrand tour\\nGraph drawing\\nHeatmap\\nHyperbolicTree\\nMultidimensional scaling\\nParallel coordinates\\nProblem solving environment\\nTreemapping\\n\\nInteractivity\\nInteractive data visualization  enables direct actions on a graphical plot to change elements and link between multiple plots.Interactive data visualization has been a pursuit of statisticians since the late 1960s. Examples of the developments can be found on the American Statistical Association video lending library.Common interactions include:\\n\\nBrushing: works by using the mouse to control a paintbrush, directly changing the color or glyph of elements of a plot. The paintbrush is sometimes a pointer and sometimes works by drawing an outline of sorts around points; the outline is sometimes irregularly shaped, like a lasso. Brushing is most commonly used when multiple plots are visible and some linking mechanism exists between the plots. There are several different conceptual models for brushing and a number of common linking mechanisms. Brushing scatterplots can be a transient operation in which points in the active plot only retain their new characteristics. At the same time, they are enclosed or intersected by the brush, or it can be a persistent operation, so that points retain their new appearance after the brush has been moved away. Transient brushing is usually chosen for linked brushing, as we have just described.\\nPainting: Persistent brushing is useful when we want to group the points into clusters and then proceed to use other operations, such as the tour, to compare the groups. It is becoming common terminology to call the persistent operation painting,\\nIdentification: which could also be called labeling or label brushing, is another plot manipulation that can be linked.  Bringing the cursor near a point or edge in a scatterplot, or a bar in a barchart, causes a label to appear that identifies the plot element. It is widely available in many interactive graphics, and is sometimes called mouseover.\\nScaling: maps the data onto the window, and changes  in the area of the.  mapping function help us learn different things from the same plot.  Scaling is commonly used to zoom in on crowded regions of a scatterplot, and it can also be used to change the aspect ratio of a plot, to reveal different features of the data.\\nLinking: connects elements selected in one plot with elements in another plot. The simplest kind of linking, one-to-one, where both plots show different projections of the same data, and a point in one plot corresponds to exactly one point in the other.  When using area plots, brushing any part of an area has the same effect as brushing it all and is equivalent to selecting all cases in the corresponding category.  Even when some plot elements represent more than one case, the underlying linking rule still links one case in one plot to the same case in other plots.  Linking can also be by categorical variable, such as by a subject id, so that all data values corresponding to that subject are highlighted, in all the visible plots.\\n\\nOther perspectives\\nThere are different approaches on the scope of data visualization. One common focus is on information presentation, such as Friedman (2008). Friendly (2008) presumes two main parts of data visualization: statistical graphics, and thematic cartography. In this line the \"Data Visualization: Modern Approaches\" (2007) article gives an overview of seven subjects of data visualization:\\nArticles & resources\\nDisplaying connections\\nDisplaying data\\nDisplaying news\\nDisplaying websites\\nMind maps\\nTools and servicesAll these subjects are closely related to graphic design and information representation.\\nOn the other hand, from a computer science perspective, Frits H. Post in 2002 categorized the field into sub-fields:\\nInformation visualization\\nInteraction techniques and architectures\\nModelling techniques\\nMultiresolution methods\\nVisualization algorithms and techniques\\nVolume visualizationWithin The Harvard Business Review, Scott Berinato developed a framework to approach data visualisation. To start thinking visually, users must consider two questions; 1) What you have and 2) what you\\'re doing. The first step is identifying what data you want visualised. It is data-driven like profit over the past ten years or a conceptual idea like how a specific organisation is structured. Once this question is answered one can then focus on whether they are trying to communicate information (declarative visualisation) or trying to figure something out (exploratory visualisation). Scott Berinato combines these questions to give four types of visual communication that each have their own goals.These four types of visual communication are as follows;\\n\\nidea illustration (conceptual & declarative).Used to teach, explain and/or simply concepts. For example, organisation charts and decision trees.\\nidea generation (conceptual & exploratory).Used to discover, innovate and solve problems. For example, a whiteboard after a brainstorming session.\\nvisual discovery (data-driven & exploratory).Used to spot trends and make sense of data. This type of visual is more common with large and complex data where the dataset is somewhat unknown and the task is open-ended.\\neveryday data-visualisation (data-driven & declarative).The most common and simple type of visualisation used for affirming and setting context. For example, a line graph of GDP over time.\\n\\nApplications\\nData and information visualization insights are being applied in areas such as:\\nScientific research\\nDigital libraries\\nData mining\\nInformation graphics\\nFinancial data analysis\\nHealth care\\nMarket studies\\nManufacturing production control\\nCrime mapping\\neGovernance and Policy Modeling\\n\\nOrganization\\nNotable academic and industry laboratories in the field are:\\n\\nAdobe Research\\nIBM Research\\nGoogle Research\\nMicrosoft Research\\nPanopticon Software\\nScientific Computing and Imaging Institute\\nTableau Software\\nUniversity of Maryland Human-Computer Interaction Lab\\nVviConferences in this field, ranked by significance in data visualization research, are:\\n\\nIEEE Visualization: An annual international conference on scientific visualization, information visualization, and visual analytics. Conference is held in October.\\nACM SIGGRAPH: An annual international conference on computer graphics, convened by the ACM SIGGRAPH organization. Conference dates vary.\\nEuroVis: An annual Europe-wide conference on data visualization, organized by the Eurographics Working Group on Data Visualization and supported by the IEEE Visualization and Graphics Technical Committee (IEEE VGTC). Conference is usually held in June.\\nConference on Human Factors in Computing Systems (CHI): An annual international conference on human–computer interaction, hosted by ACM SIGCHI. Conference is usually held in April or May.\\nEurographics: An annual Europe-wide computer graphics conference, held by the European Association for Computer Graphics. Conference is usually held in April or May.\\nPacificVis: An annual visualization symposium held in the Asia-Pacific region, sponsored by the IEEE Visualization and Graphics Technical Committee (IEEE VGTC). Conference is usually held in March or April.For further examples, see: Category:Computer graphics organizations\\n\\nData presentation architecture\\nData presentation architecture (DPA) is a skill-set that seeks to identify, locate, manipulate, format and present data in such a way as to optimally communicate meaning and proper knowledge.\\nHistorically, the term data presentation architecture is attributed to Kelly Lautt: \"Data Presentation Architecture (DPA) is a rarely applied skill set critical for the success and value of Business Intelligence. Data presentation architecture weds the science of numbers, data and statistics in discovering valuable information from data and making it usable, relevant and actionable with the arts of data visualization, communications, organizational psychology and change management in order to provide business intelligence solutions with the data scope, delivery timing, format and visualizations that will most effectively support and drive operational, tactical and strategic behaviour toward understood business (or organizational) goals. DPA is neither an IT nor a business skill set but exists as a separate field of expertise. Often confused with data visualization, data presentation architecture is a much broader skill set that includes determining what data on what schedule and in what exact format is to be presented, not just the best way to present data that has already been chosen. Data visualization skills are one element of DPA.\"\\n\\nObjectives\\nDPA has two main objectives:\\n\\nTo use data to provide knowledge in the most efficient manner possible (minimize noise, complexity, and unnecessary data or detail given each audience\\'s needs and roles)\\nTo use data to provide knowledge in the most effective manner possible  (provide relevant, timely and complete data to each audience member in a clear and understandable manner that conveys important meaning, is actionable and can affect understanding, behavior and decisions)\\n\\nScope\\nWith the above objectives in mind, the actual work of data presentation architecture consists of:\\n\\nCreating effective delivery mechanisms for each audience member depending on their role, tasks, locations and access to technology\\nDefining important meaning (relevant knowledge) that is needed by each audience member in each context\\nDetermining the required periodicity of data updates (the currency of the data)\\nDetermining the right timing for data presentation (when and how often the user needs to see the data)\\nFinding the right data (subject area, historical reach, breadth, level of detail, etc.)\\nUtilizing appropriate analysis, grouping, visualization, and other presentation formats\\n\\nRelated fields\\nDPA work shares commonalities with several other fields, including:\\n\\nBusiness analysis in determining business goals, collecting requirements, mapping processes.\\nBusiness process improvement in that its goal is to improve and streamline actions and decisions in furtherance of business goals\\nData visualization in that it uses well-established theories of visualization to add or highlight meaning or importance in data presentation.\\nDigital humanities explores more nuanced ways of visualising complex data.\\nInformation architecture, but information architecture\\'s focus is on unstructured data and therefore excludes both analysis (in the statistical/data sense) and direct transformation of the actual content (data, for DPA) into new entities and combinations.\\nHCI and interaction design, since many of the principles in how to design interactive data visualisation have been developed cross-disciplinary with HCI.\\nVisual journalism and data-driven journalism or data journalism: Visual journalism is concerned with all types of graphic facilitation of the telling of news stories, and data-driven and data journalism are not necessarily told with data visualisation. Nevertheless, the field of journalism is at the forefront in developing new data visualisations to communicate data.\\nGraphic design, conveying information through styling, typography, position, and other aesthetic concerns.\\n\\nSee also\\nNotes\\nReferences\\nFurther reading\\nCleveland, William S. (1993). Visualizing Data. Hobart Press. ISBN 0-9634884-0-6.\\nEvergreen, Stephanie (2016). Effective Data Visualization: The Right Chart for the Right Data. Sage. ISBN 978-1-5063-0305-5.\\nHealy, Kieran (2019). Data Visualization: A Practical Introduction. Princeton: Princeton University Press. ISBN 978-0-691-18161-5.\\nPost, Frits H.; Nielson, Gregory M.; Bonneau, Georges-Pierre (2003). Data Visualization: The State of the Art. New York: Springer. ISBN 978-1-4613-5430-7.\\nWilke, Claus O. (2018). Fundamentals of Data Visualization. O\\'Reilly. ISBN 978-1-4920-3108-6.\\nWilkinson, Leland (2012). Grammar of Graphics. New York: Springer. ISBN 978-1-4419-2033-1.\\nBen Bederson and Ben Shneiderman (2003). The Craft of Information Visualization: Readings and Reflections. Morgan Kaufmann.\\nStuart K. Card, Jock D. Mackinlay and Ben Shneiderman (1999). Readings in Information Visualization: Using Vision to Think, Morgan Kaufmann Publishers.\\nJeffrey Heer, Stuart K. Card, James Landay (2005). \"Prefuse: a toolkit for interactive information visualization\". In: ACM Human Factors in Computing Systems CHI 2005.\\nAndreas Kerren, John T. Stasko, Jean-Daniel Fekete, and Chris North (2008). Information Visualization – Human-Centered Issues and Perspectives. Volume 4950 of LNCS State-of-the-Art Survey, Springer.\\nRiccardo Mazza (2009). Introduction to Information Visualization, Springer.\\nSpence, Robert Information Visualization: Design for Interaction (2nd Edition), Prentice Hall, 2007, ISBN 0-13-206550-9.\\nColin Ware (2000). Information Visualization: Perception for design. San Francisco, CA: Morgan Kaufmann.\\nKawa Nazemi (2014). Adaptive Semantics Visualization Eurographics Association.\\n\\nExternal links\\n\\nMilestones in the History of Thematic Cartography, Statistical Graphics, and Data Visualization, An illustrated chronology of innovations by Michael Friendly and Daniel J. Denis.\\nDuke University-Christa Kelleher Presentation-Communicating through infographics-visualizing scientific & engineering information-March 6, 2015']]\n",
            "['Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.\\n\\nIntroduction\\nDue to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.\\nBasic reinforcement learning is modeled as a Markov decision process (MDP):\\n\\na set of environment and agent states, S;\\na set of actions, A, of the agent;\\n\\n  \\n    \\n      \\n        \\n          P\\n          \\n            a\\n          \\n        \\n        (\\n        s\\n        ,\\n        \\n          s\\n          ′\\n        \\n        )\\n        =\\n        Pr\\n        (\\n        \\n          s\\n          \\n            t\\n            +\\n            1\\n          \\n        \\n        =\\n        \\n          s\\n          ′\\n        \\n        ∣\\n        \\n          s\\n          \\n            t\\n          \\n        \\n        =\\n        s\\n        ,\\n        \\n          a\\n          \\n            t\\n          \\n        \\n        =\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle P_{a}(s,s\\')=\\\\Pr(s_{t+1}=s\\'\\\\mid s_{t}=s,a_{t}=a)}\\n   is the probability of transition (at time \\n  \\n    \\n      \\n        t\\n      \\n    \\n    {\\\\displaystyle t}\\n  ) from state \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n   to state \\n  \\n    \\n      \\n        \\n          s\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle s\\'}\\n   under action \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  .\\n\\n  \\n    \\n      \\n        \\n          R\\n          \\n            a\\n          \\n        \\n        (\\n        s\\n        ,\\n        \\n          s\\n          ′\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle R_{a}(s,s\\')}\\n   is the immediate reward after transition from \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n   to \\n  \\n    \\n      \\n        \\n          s\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle s\\'}\\n   with action \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  .The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the \"reward function\" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.A basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time t, the agent receives the current state \\n  \\n    \\n      \\n        \\n          s\\n          \\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{t}}\\n   and reward \\n  \\n    \\n      \\n        \\n          r\\n          \\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{t}}\\n  . It then chooses an action \\n  \\n    \\n      \\n        \\n          a\\n          \\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{t}}\\n   from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \\n  \\n    \\n      \\n        \\n          s\\n          \\n            t\\n            +\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{t+1}}\\n   and the reward \\n  \\n    \\n      \\n        \\n          r\\n          \\n            t\\n            +\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{t+1}}\\n   associated with the transition \\n  \\n    \\n      \\n        (\\n        \\n          s\\n          \\n            t\\n          \\n        \\n        ,\\n        \\n          a\\n          \\n            t\\n          \\n        \\n        ,\\n        \\n          s\\n          \\n            t\\n            +\\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (s_{t},a_{t},s_{t+1})}\\n   is determined. The goal of a reinforcement learning agent is to learn a policy: \\n  \\n    \\n      \\n        π\\n        :\\n        A\\n        ×\\n        S\\n        →\\n        [\\n        0\\n        ,\\n        1\\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\pi :A\\\\times S\\\\rightarrow [0,1]}\\n  , \\n  \\n    \\n      \\n        π\\n        (\\n        a\\n        ,\\n        s\\n        )\\n        =\\n        Pr\\n        (\\n        \\n          a\\n          \\n            t\\n          \\n        \\n        =\\n        a\\n        ∣\\n        \\n          s\\n          \\n            t\\n          \\n        \\n        =\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (a,s)=\\\\Pr(a_{t}=a\\\\mid s_{t}=s)}\\n   which maximizes the expected cumulative reward.\\nFormulating the problem as an MDP assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\\nWhen the agent\\'s performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.\\nThus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers and Go (AlphaGo).\\nTwo elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:\\n\\nA model of the environment is known, but an analytic solution is not available;\\nOnly a simulation model of the environment is given (the subject of simulation-based optimization);\\nThe only way to collect information about the environment is to interact with it.The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\\n\\nExploration\\nThe exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space MDPs in Burnetas and Katehakis (1997).Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite MDPs is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\\nOne such method is \\n  \\n    \\n      \\n        ε\\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon }\\n  -greedy, where \\n  \\n    \\n      \\n        0\\n        <\\n        ε\\n        <\\n        1\\n      \\n    \\n    {\\\\displaystyle 0<\\\\varepsilon <1}\\n   is a parameter controlling the amount of exploration vs. exploitation.  With probability \\n  \\n    \\n      \\n        1\\n        −\\n        ε\\n      \\n    \\n    {\\\\displaystyle 1-\\\\varepsilon }\\n  , exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \\n  \\n    \\n      \\n        ε\\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon }\\n  , exploration is chosen, and the action is chosen uniformly at random. \\n  \\n    \\n      \\n        ε\\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon }\\n   is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.\\n\\nAlgorithms for control learning\\nEven if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\\n\\nCriterion of optimality\\nPolicy\\nThe agent\\'s action selection is modeled as a map called policy:\\n\\n  \\n    \\n      \\n        π\\n        :\\n        A\\n        ×\\n        S\\n        →\\n        [\\n        0\\n        ,\\n        1\\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\pi :A\\\\times S\\\\rightarrow [0,1]}\\n  \\n\\n  \\n    \\n      \\n        π\\n        (\\n        a\\n        ,\\n        s\\n        )\\n        =\\n        Pr\\n        (\\n        \\n          a\\n          \\n            t\\n          \\n        \\n        =\\n        a\\n        ∣\\n        \\n          s\\n          \\n            t\\n          \\n        \\n        =\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (a,s)=\\\\Pr(a_{t}=a\\\\mid s_{t}=s)}\\n  The policy map gives the probability of taking action \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   when in state \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  .:\\u200a61\\u200a There are also deterministic policies.\\n\\nState-value function\\nThe value function \\n  \\n    \\n      \\n        \\n          V\\n          \\n            π\\n          \\n        \\n        (\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle V_{\\\\pi }(s)}\\n   is defined as, expected return starting with state \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  , i.e. \\n  \\n    \\n      \\n        \\n          s\\n          \\n            0\\n          \\n        \\n        =\\n        s\\n      \\n    \\n    {\\\\displaystyle s_{0}=s}\\n  , and successively following policy \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n  . Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.:\\u200a60\\u200a\\n\\n  \\n    \\n      \\n        \\n          V\\n          \\n            π\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        E\\n        \\u2061\\n        [\\n        R\\n        ∣\\n        \\n          s\\n          \\n            0\\n          \\n        \\n        =\\n        s\\n        ]\\n        =\\n        E\\n        \\u2061\\n        \\n          [\\n          \\n            \\n              ∑\\n              \\n                t\\n                =\\n                0\\n              \\n              \\n                ∞\\n              \\n            \\n            \\n              γ\\n              \\n                t\\n              \\n            \\n            \\n              r\\n              \\n                t\\n              \\n            \\n            ∣\\n            \\n              s\\n              \\n                0\\n              \\n            \\n            =\\n            s\\n          \\n          ]\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle V_{\\\\pi }(s)=\\\\operatorname {E} [R\\\\mid s_{0}=s]=\\\\operatorname {E} \\\\left[\\\\sum _{t=0}^{\\\\infty }\\\\gamma ^{t}r_{t}\\\\mid s_{0}=s\\\\right],}\\n  where the random variable \\n  \\n    \\n      \\n        R\\n      \\n    \\n    {\\\\displaystyle R}\\n   denotes the return, and is defined as the sum of future discounted rewards:\\n\\n  \\n    \\n      \\n        R\\n        =\\n        \\n          ∑\\n          \\n            t\\n            =\\n            0\\n          \\n          \\n            ∞\\n          \\n        \\n        \\n          γ\\n          \\n            t\\n          \\n        \\n        \\n          r\\n          \\n            t\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle R=\\\\sum _{t=0}^{\\\\infty }\\\\gamma ^{t}r_{t},}\\n  where \\n  \\n    \\n      \\n        \\n          r\\n          \\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{t}}\\n   is the reward at step \\n  \\n    \\n      \\n        t\\n      \\n    \\n    {\\\\displaystyle t}\\n  , \\n  \\n    \\n      \\n        γ\\n        ∈\\n        [\\n        0\\n        ,\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\gamma \\\\in [0,1)}\\n   is the discount-rate. Gamma is less than 1, so events in the distant future are weighted less than events in the immediate future.\\nThe algorithm must find a policy with maximum expected return. From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent\\'s history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\\n\\nBrute force\\nThe brute force approach entails two steps:\\n\\nFor each possible policy, sample returns while following it\\nChoose the policy with the largest expected returnOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the return of each policy.\\nThese problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\\n\\nValue function\\nValue function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\\nThese methods rely on the theory of Markov decision processes, where optimality is defined in a sense that is stronger than the above one: A policy is called optimal if it achieves the best-expected return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found amongst stationary policies.\\nTo define optimality in a formal manner, define the value of a policy \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   by\\n\\n  \\n    \\n      \\n        \\n          V\\n          \\n            π\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        E\\n        [\\n        R\\n        ∣\\n        s\\n        ,\\n        π\\n        ]\\n        ,\\n      \\n    \\n    {\\\\displaystyle V^{\\\\pi }(s)=E[R\\\\mid s,\\\\pi ],}\\n  where \\n  \\n    \\n      \\n        R\\n      \\n    \\n    {\\\\displaystyle R}\\n   stands for the return associated with following \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   from the initial state \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  . Defining \\n  \\n    \\n      \\n        \\n          V\\n          \\n            ∗\\n          \\n        \\n        (\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle V^{*}(s)}\\n   as the maximum possible value of \\n  \\n    \\n      \\n        \\n          V\\n          \\n            π\\n          \\n        \\n        (\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle V^{\\\\pi }(s)}\\n  , where \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   is allowed to change,\\n\\n  \\n    \\n      \\n        \\n          V\\n          \\n            ∗\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        \\n          max\\n          \\n            π\\n          \\n        \\n        \\n          V\\n          \\n            π\\n          \\n        \\n        (\\n        s\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle V^{*}(s)=\\\\max _{\\\\pi }V^{\\\\pi }(s).}\\n  A policy that achieves these optimal values in each state is called optimal. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return \\n  \\n    \\n      \\n        \\n          ρ\\n          \\n            π\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\rho ^{\\\\pi }}\\n  , since \\n  \\n    \\n      \\n        \\n          ρ\\n          \\n            π\\n          \\n        \\n        =\\n        E\\n        [\\n        \\n          V\\n          \\n            π\\n          \\n        \\n        (\\n        S\\n        )\\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\rho ^{\\\\pi }=E[V^{\\\\pi }(S)]}\\n  , where \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   is a state randomly sampled from the distribution \\n  \\n    \\n      \\n        μ\\n      \\n    \\n    {\\\\displaystyle \\\\mu }\\n   of initial states (so \\n  \\n    \\n      \\n        μ\\n        (\\n        s\\n        )\\n        =\\n        Pr\\n        (\\n        \\n          s\\n          \\n            0\\n          \\n        \\n        =\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\mu (s)=\\\\Pr(s_{0}=s)}\\n  ).\\nAlthough state-values suffice to define optimality, it is useful to define action-values. Given a state \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  , an action \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   and a policy \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n  , the action-value of the pair \\n  \\n    \\n      \\n        (\\n        s\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle (s,a)}\\n   under \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   is defined by\\n\\n  \\n    \\n      \\n        \\n          Q\\n          \\n            π\\n          \\n        \\n        (\\n        s\\n        ,\\n        a\\n        )\\n        =\\n        E\\n        \\u2061\\n        [\\n        R\\n        ∣\\n        s\\n        ,\\n        a\\n        ,\\n        π\\n        ]\\n        ,\\n        \\n      \\n    \\n    {\\\\displaystyle Q^{\\\\pi }(s,a)=\\\\operatorname {E} [R\\\\mid s,a,\\\\pi ],\\\\,}\\n  where \\n  \\n    \\n      \\n        R\\n      \\n    \\n    {\\\\displaystyle R}\\n   now stands for the random return associated with first taking action \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   in state \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n   and following \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n  , thereafter.\\nThe theory of MDPs states that if \\n  \\n    \\n      \\n        \\n          π\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\pi ^{*}}\\n   is an optimal policy, we act optimally (take the optimal action) by choosing the action from \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            \\n              π\\n              \\n                ∗\\n              \\n            \\n          \\n        \\n        (\\n        s\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle Q^{\\\\pi ^{*}}(s,\\\\cdot )}\\n   with the highest value at each state, \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  . The action-value function of such an optimal policy (\\n  \\n    \\n      \\n        \\n          Q\\n          \\n            \\n              π\\n              \\n                ∗\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q^{\\\\pi ^{*}}}\\n  ) is called the optimal action-value function and is commonly denoted by \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q^{*}}\\n  . In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\\nAssuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{k}}\\n   (\\n  \\n    \\n      \\n        k\\n        =\\n        0\\n        ,\\n        1\\n        ,\\n        2\\n        ,\\n        …\\n      \\n    \\n    {\\\\displaystyle k=0,1,2,\\\\ldots }\\n  ) that converge to \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q^{*}}\\n  . Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) MDPs. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\\n\\nMonte Carlo methods\\nMonte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement.\\nMonte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy \\n  \\n    \\n      \\n        π\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n  , the goal is to compute the function values \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            π\\n          \\n        \\n        (\\n        s\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle Q^{\\\\pi }(s,a)}\\n   (or a good approximation to them) for all state-action pairs \\n  \\n    \\n      \\n        (\\n        s\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle (s,a)}\\n  . Assume (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair \\n  \\n    \\n      \\n        (\\n        s\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle (s,a)}\\n   can be computed by averaging the sampled returns that originated from \\n  \\n    \\n      \\n        (\\n        s\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle (s,a)}\\n   over time.  Given sufficient time, this procedure can thus construct a precise estimate \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n   of the action-value function \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            π\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q^{\\\\pi }}\\n  . This finishes the description of the policy evaluation step.\\nIn the policy improvement step, the next policy is obtained by computing a greedy policy with respect to \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n  : Given a state \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  , this new policy returns an action that maximizes \\n  \\n    \\n      \\n        Q\\n        (\\n        s\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle Q(s,\\\\cdot )}\\n  . In practice lazy evaluation can defer the computation of the maximizing actions to when they are needed.\\nProblems with this procedure include:\\n1. The procedure may spend too much time evaluating a suboptimal policy.\\n2. It uses samples inefficiently in that a long trajectory improves the estimate only of the single state-action pair that started the trajectory.\\n3. When the returns along the trajectories have high variance, convergence is slow.\\n4. It works in episodic problems only.\\n5. It works in small, finite MDPs only.\\n\\nTemporal difference methods\\nThe first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.\\nThe second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton\\'s temporal difference (TD) methods that are based on the recursive Bellman equation. The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method, may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\\nAnother problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \\n  \\n    \\n      \\n        λ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   parameter \\n  \\n    \\n      \\n        (\\n        0\\n        ≤\\n        λ\\n        ≤\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (0\\\\leq \\\\lambda \\\\leq 1)}\\n   that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\\n\\nFunction approximation methods\\nIn order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \\n  \\n    \\n      \\n        ϕ\\n      \\n    \\n    {\\\\displaystyle \\\\phi }\\n   that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \\n  \\n    \\n      \\n        (\\n        s\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle (s,a)}\\n   are obtained by linearly combining the components of \\n  \\n    \\n      \\n        ϕ\\n        (\\n        s\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\phi (s,a)}\\n   with some weights \\n  \\n    \\n      \\n        θ\\n      \\n    \\n    {\\\\displaystyle \\\\theta }\\n  :\\n\\n  \\n    \\n      \\n        Q\\n        (\\n        s\\n        ,\\n        a\\n        )\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            d\\n          \\n        \\n        \\n          θ\\n          \\n            i\\n          \\n        \\n        \\n          ϕ\\n          \\n            i\\n          \\n        \\n        (\\n        s\\n        ,\\n        a\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle Q(s,a)=\\\\sum _{i=1}^{d}\\\\theta _{i}\\\\phi _{i}(s,a).}\\n  The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\\nValue iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants. Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\\n\\nDirect policy search\\nAn alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\\nGradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \\n  \\n    \\n      \\n        θ\\n      \\n    \\n    {\\\\displaystyle \\\\theta }\\n  , let \\n  \\n    \\n      \\n        \\n          π\\n          \\n            θ\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\pi _{\\\\theta }}\\n   denote the policy associated to \\n  \\n    \\n      \\n        θ\\n      \\n    \\n    {\\\\displaystyle \\\\theta }\\n  . Defining the performance function by\\n\\n  \\n    \\n      \\n        ρ\\n        (\\n        θ\\n        )\\n        =\\n        \\n          ρ\\n          \\n            \\n              π\\n              \\n                θ\\n              \\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\rho (\\\\theta )=\\\\rho ^{\\\\pi _{\\\\theta }},}\\n  under mild conditions this function will be differentiable as a function of the parameter vector \\n  \\n    \\n      \\n        θ\\n      \\n    \\n    {\\\\displaystyle \\\\theta }\\n  . If the gradient of \\n  \\n    \\n      \\n        ρ\\n      \\n    \\n    {\\\\displaystyle \\\\rho }\\n   was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams\\' REINFORCE method (which is known as the likelihood ratio method in the simulation-based optimization literature). Policy search methods have been used in the robotics context. Many policy search methods may get stuck in local optima (as they are based on local search).\\nA large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\\nPolicy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.\\n\\nModel-based algorithms\\nFinally, all of the above methods can be combined with algorithms that first learn a model.  For instance, the Dyna algorithm learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and \\'replayed\\' to the learning algorithm.\\nThere are other ways to use models than to update a value function. For instance, in model predictive control the model is used to update the behavior directly.\\n\\nTheory\\nBoth the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\\nEfficient exploration of MDPs is given in  Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\\nFor incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\\n\\nResearch\\nResearch topics include: \\n\\nactor-critic\\nadaptive methods that work with fewer (or no) parameters under a large number of conditions\\nbug detection in software projects\\ncontinuous learning\\ncombinations with logic-based frameworks\\nexploration in large MDPs\\nhuman feedback\\ninteraction between implicit and explicit learning in skill acquisition\\nintrinsic motivation which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations\\nlarge (or continuous) action spaces\\nmodular and hierarchical reinforcement learning\\nmultiagent/distributed reinforcement learning is a topic of interest. Applications are expanding.\\noccupant-centric control\\noptimization of computing resources\\npartial information (e.g., using predictive state representation)\\nreward function based on maximising novel information\\nsample-based planning (e.g., based on Monte Carlo tree search).\\nsecurities trading\\ntransfer learning\\nTD learning modeling dopamine-based learning in the brain. Dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error.\\nvalue-function and policy search methods\\n\\nComparison of reinforcement learning algorithms\\nAssociative reinforcement learning\\nAssociative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.\\n\\nDeep reinforcement learning\\nThis approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.\\n\\nAdversarial deep reinforcement learning\\nAdversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.\\n\\nFuzzy reinforcement learning\\nBy introducing fuzzy inference in RL, approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation  allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\\n\\nInverse reinforcement learning\\nIn inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.\\n\\nSafe reinforcement learning\\nSafe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.\\n\\nSee also\\nReferences\\nFurther reading\\nAuer, Peter; Jaksch, Thomas; Ortner, Ronald (2010). \"Near-optimal regret bounds for reinforcement learning\". Journal of Machine Learning Research. 11: 1563–1600.\\nBusoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010). Reinforcement Learning and Dynamic Programming using Function Approximators. Taylor & Francis CRC Press. ISBN 978-1-4398-2108-4.\\nFrançois-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). \"An Introduction to Deep Reinforcement Learning\". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. Bibcode:2018arXiv181112560F. doi:10.1561/2200000071. S2CID 54434537.\\nPowell, Warren (2011). Approximate dynamic programming: solving the curses of dimensionality. Wiley-Interscience.\\nSutton, Richard S.; Barto, Andrew G. (2018). Reinforcement Learning: An Introduction (2 ed.). MIT Press. ISBN 978-0-262-03924-6.\\nSutton, Richard S. (1988). \"Learning to predict by the method of temporal differences\". Machine Learning. 3: 9–44. doi:10.1007/BF00115009.\\nSzita, Istvan; Szepesvari, Csaba (2010). \"Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds\" (PDF). ICML 2010. Omnipress. pp. 1031–1038. Archived from the original (PDF) on 2010-07-14.\\n\\nExternal links\\nReinforcement Learning Repository\\nReinforcement Learning and Artificial Intelligence (RLAI, Rich Sutton\\'s lab at the University of Alberta)\\nAutonomous Learning Laboratory (ALL, Andrew Barto\\'s lab at the University of Massachusetts Amherst)\\nReal-world reinforcement learning experiments Archived 2018-10-08 at the Wayback Machine at Delft University of Technology\\nStanford University Andrew Ng Lecture on Reinforcement Learning\\nDissecting Reinforcement Learning Series of blog post on RL with Python code\\nA (Long) Peek into Reinforcement Learning', 'Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.\\nAfter cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.\\nThe actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of \"varying file formats, naming conventions, and columns\", and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (\"st, rd, etc.\" to \"street, road, etcetera\").\\n\\nMotivation\\nAdministratively incorrect, inconsistent data can lead to false conclusions and misdirect investments on both public and private scales. For instance, the government may want to analyze population census figures to decide which regions require further spending and investment on infrastructure and services. In this case, it will be important to have access to reliable data to avoid erroneous fiscal decisions. In the business world, incorrect data can be costly. Many companies use customer information databases that record data like contact information, addresses, and preferences. For instance, if the addresses are inconsistent, the company will suffer the cost of resending mail or even losing customers.\\n\\nData quality\\nHigh-quality data needs to pass a set of quality criteria. Those include:\\n\\nValidity: The degree to which the measures conform to defined business rules or constraints (see also Validity (statistics)). When modern database technology is used to design data-capture systems, validity is fairly easy to ensure: invalid data arises mainly in legacy contexts (where constraints were not implemented in software) or where inappropriate data-capture technology was used (e.g., spreadsheets, where it is very hard to limit what a user chooses to enter into a cell, if cell validation is not used). Data constraints fall into the following categories:\\nData-Type Constraints – e.g., values in a particular column must be of a particular data type, e.g., Boolean, numeric (integer or real), date, etc.\\nRange Constraints: typically, numbers or dates should fall within a certain range. That is, they have minimum and/or maximum permissible values.\\nMandatory Constraints: Certain columns cannot be empty.\\nUnique Constraints: A field, or a combination of fields, must be unique across a dataset. For example, no two persons can have the same social security number.\\nSet-Membership constraints: The values for a column come from a set of discrete values or codes. For example, a person\\'s sex may be Female, Male or Non-Binary.\\nForeign-key constraints: This is the more general case of set membership. The set of values in a column is defined in a column of another table that contains unique values. For example, in a US taxpayer database, the \"state\" column is required to belong to one of the US\\'s defined states or territories: the set of permissible states/territories is recorded in a separate State table. The term foreign key is borrowed from relational database terminology.\\nRegular expression patterns: Occasionally, text fields will have to be validated this way. For example, phone numbers may be required to have the pattern (999) 999–9999.\\nCross-field validation: Certain conditions that utilize multiple fields must hold. For example, in laboratory medicine, the sum of the components of the differential white blood cell count must be equal to 100 (since they are all percentages). In a hospital database, a patient\\'s date of discharge from the hospital cannot be earlier than the date of admission.\\nAccuracy: The degree of conformity of a measure to a standard or a true value - see also Accuracy and precision. Accuracy is very hard to achieve through data-cleansing in the general case because it requires accessing an external source of data that contains the true value: such \"gold standard\" data is often unavailable. Accuracy has been achieved in some cleansing contexts, notably customer contact data, by using external databases that match up zip codes to geographical locations (city and state) and also help verify that street addresses within these zip codes actually exist.\\nCompleteness: The degree to which all required measures are known. Incompleteness is almost impossible to fix with data cleansing methodology: one cannot infer facts that were not captured when the data in question was initially recorded. (In some contexts, e.g., interview data, it may be possible to fix incompleteness by going back to the original source of data, i.e. re-interviewing the subject, but even this does not guarantee success because of problems of recall - e.g., in an interview to gather data on food consumption, no one is likely to remember exactly what one ate six months ago. In the case of systems that insist certain columns should not be empty, one may work around the problem by designating a value that indicates \"unknown\" or \"missing\", but the supplying of default values does not imply that the data has been made complete.)\\nConsistency: The degree to which a set of measures are equivalent in across systems (see also Consistency). Inconsistency occurs when two data items in the data set contradict each other: e.g., a customer is recorded in two different systems as having two different current addresses, and only one of them can be correct. Fixing inconsistency is not always possible: it requires a variety of strategies - e.g., deciding which data were recorded more recently, which data source is likely to be most reliable (the latter knowledge may be specific to a given organization), or simply trying to find the truth by testing both data items (e.g., calling up the customer).\\nUniformity: The degree to which a set data measures are specified using the same units of measure in all systems ( see also Unit of measure). In datasets pooled from different locales, weight may be recorded either in pounds or kilos and must be converted to a single measure using an arithmetic transformation.The term integrity encompasses accuracy, consistency and some aspects of validation (see also data integrity) but is rarely used by itself in data-cleansing contexts because it is insufficiently specific. (For example, \"referential integrity\" is a term used to refer to the enforcement of foreign-key constraints above.)\\n\\nProcess\\nData auditing: The data is audited with the use of statistical and database methods to detect anomalies and contradictions: this eventually indicates the characteristics of the anomalies and their locations. Several commercial software packages will let you specify constraints of various kinds (using a grammar that conforms to that of a standard programming language, e.g., JavaScript or Visual Basic) and then generate code that checks the data for violation of these constraints. This process is referred to below in the bullets \"workflow specification\" and \"workflow execution.\" For users who lack access to high-end cleansing software, Microcomputer database packages such as Microsoft Access or File Maker Pro will also let you perform such checks, on a constraint-by-constraint basis, interactively with little or no programming required in many cases.\\nWorkflow specification: The detection and removal of anomalies are performed by a sequence of operations on the data known as the workflow. It is specified after the process of auditing the data and is crucial in achieving the end product of high-quality data. In order to achieve a proper workflow, the causes of the anomalies and errors in the data have to be closely considered.\\nWorkflow execution: In this stage, the workflow is executed after its specification is complete and its correctness is verified. The implementation of the workflow should be efficient, even on large sets of data, which inevitably poses a trade-off because the execution of a data-cleansing operation can be computationally expensive.\\nPost-processing and controlling: After executing the cleansing workflow, the results are inspected to verify correctness. Data that could not be corrected during the execution of the workflow is manually corrected, if possible. The result is a new cycle in the data-cleansing process where the data is audited again to allow the specification of an additional workflow to further cleanse the data by automatic processing.Good quality source data has to do with “Data Quality Culture” and must be initiated at the top of the organization. It is not just a matter of implementing strong validation checks on input screens, because almost no matter how strong these checks are, they can often still be circumvented by the users. There is a nine-step guide for organizations that wish to improve data quality:\\nDeclare a high-level commitment to a data quality culture\\nDrive process reengineering at the executive level\\nSpend money to improve the data entry environment\\nSpend money to improve application integration\\nSpend money to change how processes work\\nPromote end-to-end team awareness\\nPromote interdepartmental cooperation\\nPublicly celebrate data quality excellence\\nContinuously measure and improve data qualityOthers include:\\n\\nParsing:  for the detection of syntax errors. A parser decides whether a string of data is acceptable within the allowed data specification. This is similar to the way a parser works with grammars and languages.\\nData transformation: Data transformation allows the mapping of the data from its given format into the format expected by the appropriate application. This includes value conversions or translation functions, as well as normalizing numeric values to conform to minimum and maximum values.\\nDuplicate elimination: Duplicate detection requires an algorithm for determining whether data contains duplicate representations of the same entity. Usually, data is sorted by a key that would bring duplicate entries closer together for faster identification.\\nStatistical methods: By analyzing the data using the values of mean, standard deviation, range, or clustering algorithms, it is possible for an expert to find values that are unexpected and thus erroneous. Although the correction of such data is difficult since the true value is not known, it can be resolved by setting the values to an average or other statistical value. Statistical methods can also be used to handle missing values which can be replaced by one or more plausible values, which are usually obtained by extensive data augmentation algorithms.\\n\\nSystem\\nThe essential job of this system is to find a suitable balance between fixing dirty data and maintaining the data as close as possible to the original data from the source production system. This is a challenge for the Extract, transform, load architect. The system should offer an architecture that can cleanse data, record quality events and measure/control quality of data in the data warehouse. A good start is to perform a thorough data profiling analysis that will help define to the required complexity of the data cleansing system and also give an idea of the current data quality in the source system(s).\\n\\nQuality screens\\nPart of the data cleansing system is a set of diagnostic filters known as quality screens. They each implement a test in the data flow that, if it fails, records an error in the Error Event Schema. Quality screens are divided into three categories:\\n\\nColumn screens. Testing the individual column, e.g. for unexpected values like NULL values; non-numeric values that should be numeric; out of range values; etc.\\nStructure screens. These are used to test for the integrity of different relationships between columns (typically foreign/primary keys) in the same or different tables. They are also used for testing that a group of columns is valid according to some structural definition to which it should adhere.\\nBusiness rule screens. The most complex of the three tests. They test to see if data, maybe across multiple tables, follow specific business rules. An example could be, that if a customer is marked as a certain type of customer, the business rules that define this kind of customer should be adhered to.When a quality screen records an error, it can either stop the dataflow process, send the faulty data somewhere else than the target system or tag the data.\\nThe latter option is considered the best solution because the first option requires, that someone has to manually deal with the issue each time it occurs and the second implies that data are missing from the target system (integrity) and it is often unclear what should happen to these data.\\n\\nCriticism of existing tools and processes\\nMost data cleansing tools have limitations in usability:\\n\\nProject costs: costs typically in the hundreds of thousands of dollars\\nTime: mastering large-scale data-cleansing software is time-consuming\\nSecurity: cross-validation requires sharing information, giving an application access across systems, including sensitive legacy systems\\n\\nError event schema\\nThe error event schema holds records of all error events thrown by the quality screens. It consists of an error event Fact table with foreign keys to three dimension tables that represent date (when), batch job (where) and screen (who produced error). It also holds information about exactly when the error occurred and the severity of the error. Also, there is an error event detail fact table with a foreign key to the main table that contains detailed information about in which table, record and field the error occurred and the error condition.\\n\\nSee also\\nData editing\\nData mining\\nDatabase repair\\nIterative proportional fitting\\nRecord linkage\\nSingle customer view\\nTriangulation (social science)\\n\\nReferences\\nFurther reading\\nMcKinney, Wes (2017). \"Data Cleaning and Preparation\". Python for Data Analysis (2nd ed.). O\\'Reilly. pp. 195–224. ISBN 978-1-4919-5766-0.\\nvan der Loo, Mark; de Jonge, Edwin (2018). Statistical Data Cleaning with Applications in R. Hoboken: Wiley. ISBN 978-1-118-89715-7.\\n\\nExternal links\\nComputerworld: Data Scrubbing (February 10, 2003)\\nErhard Rahm, Hong Hai Do: Data Cleaning: Problems and Current Approaches\\nData cleansing. Datamanagement.wiki.']\n"
          ]
        }
      ],
      "source": [
        "#!pip install wikipedia-api\n",
        "import wikipediaapi\n",
        "\n",
        "# Define the Wikipedia languages and pages to download\n",
        "LANGUAGES = ['en']\n",
        "DOMAINS = ['Machine learning', 'Data science']\n",
        "TRAIN_PAGES = [['Machine learning', 'Artificial intelligence', 'Deep learning', 'Supervised learning', 'Unsupervised learning'],\n",
        "               ['Data science', 'Data analysis', 'Data mining', 'Big data', 'Data visualization']]\n",
        "TEST_PAGES = [['Reinforcement learning'], ['Data cleaning']]\n",
        "\n",
        "# Initialize the Wikipedia API\n",
        "wiki = wikipediaapi.Wikipedia('en')\n",
        "\n",
        "# Download the training and test documents for each domain\n",
        "train_docs = []\n",
        "test_docs = []\n",
        "for i in range(len(DOMAINS)):\n",
        "    # Download the training documents\n",
        "    train_domain_docs = []\n",
        "    for train_page in TRAIN_PAGES[i]:\n",
        "        page = wiki.page(train_page)\n",
        "        if page.exists():\n",
        "            text = page.text\n",
        "            if len(text.split()) >= 100:  # Check if document has at least 100 words\n",
        "                train_domain_docs.append(text)\n",
        "    train_docs.append(train_domain_docs)\n",
        "\n",
        "    # Download the test document\n",
        "    test_page = wiki.page(TEST_PAGES[i][0])\n",
        "    if test_page.exists():\n",
        "        text = test_page.text\n",
        "        if len(text.split()) >= 100:  # Check if document has at least 100 words\n",
        "            test_docs.append(text)\n",
        "\n",
        "print(train_docs)\n",
        "print(test_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def class_prob(doc_label):\n",
        "    \"\"\"\n",
        "    Calculates the probability of each class given the document labels.\n",
        "\n",
        "    Args:\n",
        "        doc_label (list): A list of document labels.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where the keys are the unique labels in doc_label and the values are their corresponding probabilities.\n",
        "    \"\"\"\n",
        "    class_counts = {}\n",
        "    for label in doc_label:\n",
        "        if label in class_counts:\n",
        "            class_counts[label] += 1\n",
        "        else:\n",
        "            class_counts[label] = 1\n",
        "    total_docs = len(doc_label)\n",
        "    class_probs = {}\n",
        "    for label, count in class_counts.items():\n",
        "        class_probs[label] = count / total_docs\n",
        "    return class_probs\n"
      ],
      "metadata": {
        "id": "6Yxjo_3Gowxz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}